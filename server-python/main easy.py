from fastapi import FastAPI, HTTPException, Query, Depends
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field, field_serializer
from datetime import datetime
from typing import Optional, List, AsyncGenerator, Dict
from contextlib import asynccontextmanager
import os
from dotenv import load_dotenv
from sqlalchemy import create_engine, Column, Integer, String, Text, TIMESTAMP, func
from sqlalchemy.orm import declarative_base
from sqlalchemy.orm import sessionmaker, Session
import uvicorn
import feedparser
import requests
from bs4 import BeautifulSoup
import time
import re

# ê°„ë‹¨ ë²„ì „ì—ì„œëŠ” ê¸°ë³¸ ì„¸ì…˜ë§Œ ì‚¬ìš©
session = requests.Session()

def get_sort_key(article):
    """ê¸°ì‚¬ ì •ë ¬ì„ ìœ„í•œ í‚¤ í•¨ìˆ˜ - ìµœì‹ ìˆœ ì •ë ¬"""
    published_date = article.get("publishedAt", "")
    if published_date:
        try:
            # ì´ë¯¸ ISO formatì´ë¯€ë¡œ ë°”ë¡œ íŒŒì‹±
            if published_date.endswith('Z'):
                published_date = published_date.replace('Z', '+00:00')
            return datetime.fromisoformat(published_date)
        except Exception as e:
            print(f"âš ï¸ Date parsing error for article: {article.get('title', '')[:30]}... - {e}")
            return datetime.min
    return datetime.min


def decode_google_news_url(url: str, session=None) -> str:
    """ê°„ë‹¨ ë²„ì „: URL ê·¸ëŒ€ë¡œ ë°˜í™˜"""
    return url


def extract_news_content(article_url: str, session=None) -> str:
    """ê°„ë‹¨ ë²„ì „: ë³¸ë¬¸ ì¶”ì¶œ ìƒëµ"""
    return None




# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


# ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
DATABASE_URL = os.getenv("DATABASE_URL") or "sqlite:///./news.db"

engine = create_engine(DATABASE_URL, echo=True)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

# ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë¸
class Post(Base):
    __tablename__ = "posts"

    id = Column(Integer, primary_key=True, index=True)
    title = Column(String, nullable=False)
    summary = Column(Text, nullable=False)
    content = Column(Text, nullable=False)
    category = Column(String, nullable=False)
    image_url = Column("image_url", String, nullable=False)
    created_at = Column("created_at", TIMESTAMP, server_default=func.now())

# Pydantic ìŠ¤í‚¤ë§ˆ
class PostBase(BaseModel):
    title: str
    summary: str
    content: str
    category: str
    image_url: str

class PostCreate(PostBase):
    pass

class PostResponse(PostBase):
    id: int
    created_at: Optional[datetime] = None

    @field_serializer('created_at')
    def serialize_created_at(self, value: Optional[datetime]) -> Optional[str]:
        if value is None:
            return None
        return value.isoformat()

    model_config = {
        "from_attributes": True,
        "populate_by_name": True
    }


# lifespan ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
@asynccontextmanager
async def lifespan(app: FastAPI) -> AsyncGenerator[None, None]:
    # startup
    Base.metadata.create_all(bind=engine)
    await seed_database()
    yield
    # shutdown (í•„ìš”ì‹œ cleanup ì½”ë“œ ì¶”ê°€)


# FastAPI ì•± ìƒì„±
app = FastAPI(title="News API", version="1.0.0", lifespan=lifespan)

# CORS ì„¤ì • (í•„ìš”ì‹œ)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://127.0.0.1:5173"],  # Vite ê¸°ë³¸ í¬íŠ¸
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ ì˜ì¡´ì„±
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


# ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” (í…Œì´ë¸” ìƒì„±)



async def seed_database():
    db = SessionLocal()
    try:
        # ê¸°ì¡´ ë°ì´í„° ëª¨ë‘ ì‚­ì œ (ìŠ¤í‚¤ë§ˆ ë³€ê²½ìœ¼ë¡œ ì¸í•œ ë¦¬ì…‹)
        db.query(Post).delete()
        db.commit()
        print("Existing posts deleted for schema update")
        # ì‹œë“œ ë°ì´í„°
        seed_posts = [
            {
                "title": "2025ë…„ AIì˜ ë¯¸ë˜ ì „ë§",
                "summary": "ì¸ê³µì§€ëŠ¥ì´ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë‚´ë…„ì— ì–´ë–¤ ë³€í™”ê°€ ì˜ˆìƒë˜ëŠ”ì§€ ì•Œì•„ë³´ì„¸ìš”.",
                "content": "ì¸ê³µì§€ëŠ¥ì´ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë‚´ë…„ì— ì–´ë–¤ ë³€í™”ê°€ ì˜ˆìƒë˜ëŠ”ì§€ ì•Œì•„ë³´ì„¸ìš”. ì „ë¬¸ê°€ë“¤ì€ ìƒì„±í˜• ëª¨ë¸ê³¼ ììœ¨ ì—ì´ì „íŠ¸ ë¶„ì•¼ì—ì„œ ì£¼ìš” ëŒíŒŒêµ¬ë¥¼ ì˜ˆìƒí•˜ê³  ìˆìŠµë‹ˆë‹¤. AIì˜ ì¼ìƒìƒí™œ í†µí•©ì´ ë”ìš± ì›í™œí•´ì§€ë©° ì˜ë£Œ, ê¸ˆìœµ ë“± ë‹¤ì–‘í•œ ì‚°ì—…ì— ì˜í–¥ì„ ë¯¸ì¹  ê²ƒì…ë‹ˆë‹¤.",
                "category": "ê¸°ìˆ ",
                "image_url": "https://images.unsplash.com/photo-1677442136019-21780ecad995?auto=format&fit=crop&q=80&w=800",
            },
            {
                "title": "ì¸í”Œë ˆì´ì…˜ ì™„í™”ë¡œ ê¸€ë¡œë²Œ ì¦ì‹œ ìƒìŠ¹",
                "summary": "ì´ë²ˆ ì£¼ ê²½ì œ ì§€í‘œ í˜¸ì¡°ë¡œ ì£¼ì‹ ì‹œì¥ì´ ì‹ ê³ ì ì„ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤.",
                "content": "ì´ë²ˆ ì£¼ ê²½ì œ ì§€í‘œ í˜¸ì¡°ë¡œ ì£¼ì‹ ì‹œì¥ì´ ì‹ ê³ ì ì„ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤. íˆ¬ììë“¤ì€ ì¤‘ì•™ì€í–‰ì˜ ë‹¤ìŒ ì›€ì§ì„ì— ëŒ€í•´ ë‚™ê´€ì ì…ë‹ˆë‹¤. ê¸°ìˆ ì£¼ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì£¼ìš” ì§€ìˆ˜ê°€ ì‚¬ìƒ ìµœê³ ì¹˜ë¥¼ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤.",
                "category": "ë¹„ì¦ˆë‹ˆìŠ¤",
                "image_url": "https://images.unsplash.com/photo-1611974765270-ca12586343bb?auto=format&fit=crop&q=80&w=800",
            },
            {
                "title": "ìƒëª…ì¡´ì— ìœ„ì¹˜í•œ ìƒˆë¡œìš´ í–‰ì„± ë°œê²¬",
                "summary": "ì²œë¬¸í•™ìë“¤ì´ ì§€êµ¬ì™€ ìœ ì‚¬í•œ ì ì¬ì  í–‰ì„±ì„ 40ê´‘ë…„ ê±°ë¦¬ì—ì„œ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.",
                "content": "ì²œë¬¸í•™ìë“¤ì´ ì§€êµ¬ì™€ ìœ ì‚¬í•œ ì ì¬ì  í–‰ì„±ì„ 40ê´‘ë…„ ê±°ë¦¬ì—ì„œ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ê¸€ë¦¬ì œ 12 bë¡œ ëª…ëª…ëœ ì´ í–‰ì„±ì€ ì ìƒ‰ ì™œì„± ì£¼ìœ„ë¥¼ ê³µì „í•˜ë©° ì•¡ì²´ ë¬¼ì„ ìœ ì§€í•  ìˆ˜ ìˆëŠ” ì˜¨ë„ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì œì„ìŠ¤ ì›¹ ìš°ì£¼ ë§ì›ê²½ì„ í†µí•œ ì¶”ê°€ ê´€ì¸¡ì´ ê³„íšë˜ì–´ ìˆìŠµë‹ˆë‹¤.",
                "category": "ê³¼í•™",
                "image_url": "https://images.unsplash.com/photo-1451187580459-43490279c0fa?auto=format&fit=crop&q=80&w=800",
            },
            {
                "title": "ì˜¤ëŠ˜ ë°¤ ìˆ™ë©´ì„ ìœ„í•œ 5ê°€ì§€ íŒ",
                "summary": "ìˆ™ë©´ì„ ì·¨í•˜ê¸° ì–´ë ¤ìš°ì‹ ê°€ìš”? ê³¼í•™ì ìœ¼ë¡œ ê²€ì¦ëœ íŒì„ í™•ì¸í•˜ì„¸ìš”.",
                "content": "ìˆ™ë©´ì„ ì·¨í•˜ê¸° ì–´ë ¤ìš°ì‹ ê°€ìš”? ê³¼í•™ì ìœ¼ë¡œ ê²€ì¦ëœ íŒì„ í™•ì¸í•˜ì„¸ìš”. 1. ê·œì¹™ì ì¸ ì¼ì • ìœ ì§€í•˜ê¸°. 2. í¸ì•ˆí•œ í™˜ê²½ ì¡°ì„±í•˜ê¸°. 3. ì·¨ì¹¨ ì „ í™”ë©´ ì‹œê°„ ì œí•œí•˜ê¸°. 4. ë¨¹ëŠ” ìŒì‹ê³¼ ë§ˆì‹œëŠ” ìŒë£Œ ì£¼ì˜í•˜ê¸°. 5. ì¼ìƒ ìƒí™œì— ì‹ ì²´ í™œë™ í¬í•¨í•˜ê¸°.",
                "category": "ê±´ê°•",
                "image_url": "https://images.unsplash.com/photo-1541781777621-794453259724?auto=format&fit=crop&q=80&w=800",
            },
            {
                "title": "ì˜¬ì—¬ë¦„ ë³¼ë§Œí•œ ê¸°ëŒ€ì‘ ì˜í™”ë“¤",
                "summary": "íŒì½˜ ì¤€ë¹„í•˜ì„¸ìš”! ì´ë²ˆ ì‹œì¦Œ ê°€ì¥ ê¸°ëŒ€ë˜ëŠ” ì˜í™”ë“¤ì„ ì†Œê°œí•©ë‹ˆë‹¤.",
                "content": "íŒì½˜ ì¤€ë¹„í•˜ì„¸ìš”! ì´ë²ˆ ì‹œì¦Œ ê°€ì¥ ê¸°ëŒ€ë˜ëŠ” ì˜í™”ë“¤ì„ ì†Œê°œí•©ë‹ˆë‹¤. ìŠˆí¼íˆì–´ë¡œ ëŒ€ì‘ë¶€í„° ë”°ëœ»í•œ ê°ë™ ì• ë‹ˆë©”ì´ì…˜ê¹Œì§€ ëª¨ë‘ë¥¼ ìœ„í•œ ì‘í’ˆì´ ì¤€ë¹„ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ë²ˆ ì—¬ë¦„ ê·¹ì¥ì—ì„œ ë³¼ ìˆ˜ ìˆëŠ” í•„ëŒ ì˜í™” ëª©ë¡ì„ í™•ì¸í•´ë³´ì„¸ìš”.",
                "category": "ì—”í„°í…Œì¸ë¨¼íŠ¸",
                "image_url": "https://images.unsplash.com/photo-1536440136628-849c177e76a1?auto=format&fit=crop&q=80&w=800",
            },
        ]

        for post_data in seed_posts:
            post = Post(**post_data)
            db.add(post)
        db.commit()
        print("Database seeded with initial posts")
    except Exception as e: 
        db.rollback()
        print(f"Error seeding database: {e}")
    finally:
        db.close()



# Google News RSS Client
class GoogleNewsRSSClient:
    def __init__(self):
        # í•œêµ­ ë‰´ìŠ¤ RSS í”¼ë“œ
        self.base_url = "https://news.google.com/rss"
        # ê°„ë‹¨ ë²„ì „ì—ì„œëŠ” ê¸°ë³¸ ì„¸ì…˜ ì‚¬ìš©
        self.session = session

    def extract_article_content(self, url: str) -> str:
        """ê°„ë‹¨ ë²„ì „: ë³¸ë¬¸ ì¶”ì¶œ ìƒëµ"""
        return ""

    def _extract_real_url(self, google_news_url: str) -> str:
        """Google News URLì—ì„œ ì‹¤ì œ ë‰´ìŠ¤ URL ì¶”ì¶œ (ê°„ì†Œí™”ëœ ë²„ì „)"""
        # ìƒˆë¡œ ë§Œë“  ì „ë¬¸ ë””ì½”ë” ì‚¬ìš© - self.session ì „ë‹¬!
        return decode_google_news_url(google_news_url, self.session)

    def get_news_by_topic(self, topic: str = "general") -> List[Dict]:
        """Google News ê²€ìƒ‰ RSSì—ì„œ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
        # í† í”½ë³„ ê²€ìƒ‰ì–´ ë§¤í•‘ (ë” ì•ˆì •ì ì¸ ë°©ì‹)
        topic_queries = {
            "business": "ë¹„ì¦ˆë‹ˆìŠ¤ OR ê²½ì œ OR ê¸°ì—… OR ê¸ˆìœµ",
            "technology": "ê¸°ìˆ  OR IT OR ì¸ê³µì§€ëŠ¥ OR ìŠ¤íƒ€íŠ¸ì—…",
            "science": "ê³¼í•™ OR ì—°êµ¬ OR ìš°ì£¼ OR í™˜ê²½",
            "health": "ê±´ê°• OR ì˜ë£Œ OR ë³‘ì› OR ì½”ë¡œë‚˜",
            "entertainment": "ì—°ì˜ˆ OR ì˜í™” OR ìŒì•… OR ë“œë¼ë§ˆ",
            "general": ""
        }

        # Google News ê²€ìƒ‰ RSS URL ìƒì„±
        base_url = "https://news.google.com/rss/search?q="
        query = topic_queries.get(topic, "")
        if query:
            # ê²€ìƒ‰ì–´ë¥¼ URL ì¸ì½”ë”©
            import urllib.parse
            encoded_query = urllib.parse.quote(query)
            rss_url = f"{base_url}{encoded_query}&hl=ko&gl=KR&ceid=KR:ko"
        else:
            rss_url = "https://news.google.com/rss?hl=ko&gl=KR&ceid=KR:ko"

        try:
            # RSS í”¼ë“œ íŒŒì‹±
            print(f"ğŸŒ Fetching RSS from: {rss_url}")  # ë””ë²„ê¹… ë¡œê·¸

            # SSL ê²€ì¦ ì—†ì´ RSS ê°€ì ¸ì˜¤ê¸° (requests ì‚¬ìš©)
            response = self.session.get(rss_url, verify=False)
            response.raise_for_status()
            rss_content = response.text

            # ê°€ì ¸ì˜¨ RSS í…ìŠ¤íŠ¸ë¥¼ feedparserë¡œ íŒŒì‹±
            feed = feedparser.parse(rss_content)

            # ìƒì„¸í•œ ë””ë²„ê¹… ì •ë³´
            print(f"ğŸ“¡ Feed status: {feed.status if hasattr(feed, 'status') else 'unknown'}")
            print(f"ğŸ“° Feed entries count: {len(feed.entries)}")
            print(f"ğŸ“ Feed title: {getattr(feed.feed, 'title', 'No title')}")
            print(f"ğŸ” Feed keys: {list(feed.keys())}")
            print(f"ğŸ“„ Raw feed data (first 500 chars): {str(feed)[:500]}")

            if hasattr(feed, 'bozo') and feed.bozo:
                print(f"âš ï¸ Feed parsing error: {feed.bozo_exception}")

            # entries ìƒì„¸ ì •ë³´
            if feed.entries:
                print(f"âœ… First entry keys: {list(feed.entries[0].keys()) if feed.entries else 'No entries'}")
                print(f"âœ… First entry title: {getattr(feed.entries[0], 'title', 'No title') if feed.entries else 'No entries'}")
            else:
                print(f"âŒ No entries found in feed")

            articles = []
            for entry in feed.entries[:20]:  # ìµœëŒ€ 20ê°œ ë‰´ìŠ¤
                # ì´ë¯¸ì§€ URL ì¶”ì¶œ ê°œì„ 
                image_url = ""
                if hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
                    image_url = entry.media_thumbnail[0].get('url', '')
                elif hasattr(entry, 'media_content') and entry.media_content:
                    image_url = entry.media_content[0].get('url', '')
                elif hasattr(entry, 'enclosures') and entry.enclosures:
                    for enclosure in entry.enclosures:
                        if enclosure.get('type', '').startswith('image/'):
                            image_url = enclosure.get('url', '')
                            break

                # ë‚ ì§œ ì²˜ë¦¬ ê°œì„ 
                published_at = getattr(entry, 'published', '')
                if published_at:
                    try:
                        from email.utils import parsedate_to_datetime
                        published_at = parsedate_to_datetime(published_at).isoformat()
                    except:
                        published_at = datetime.now().isoformat()

                # Google News ë§í¬ì—ì„œ ì‹¤ì œ ë‰´ìŠ¤ URL ì¶”ì¶œ ì‹œë„
                actual_url = self._extract_real_url(entry.link)

                article = {
                    "title": entry.title,
                    "description": getattr(entry, 'summary', ''),
                    "content": getattr(entry, 'summary', ''),  # RSSì—ì„œëŠ” ì½˜í…ì¸ ê°€ ì œí•œì 
                    "url": actual_url,  # ì‹¤ì œ ë‰´ìŠ¤ URL ì‚¬ìš©
                    "urlToImage": image_url,
                    "publishedAt": published_at
                }
                articles.append(article)

            print(f"âœ… Returning {len(articles)} articles")
            return articles

        except Exception as e:
            print(f"ğŸ’¥ Error parsing RSS feed for {topic}: {e}")
            import traceback
            print(f"ğŸ’¥ Full traceback: {traceback.format_exc()}")
            return []
        







# News fetch, save Func
async def fetch_and_store_news(db: Session):
    """Google News RSSì—ì„œ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì™€ì„œ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
    client = GoogleNewsRSSClient()

    # ì—¬ëŸ¬ ì¹´í…Œê³ ë¦¬ì˜ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
    categories = ["business", "technology", "science", "health", "entertainment"]

    total_processed = 0
    total_saved = 0

    for category in categories:
        print(f"ğŸ” Fetching {category} news...")  # ë””ë²„ê¹… ë¡œê·¸
        articles = client.get_news_by_topic(topic=category)
        print(f"ğŸ“Š Found {len(articles)} articles for {category}")  # ë””ë²„ê¹… ë¡œê·¸

        # ìµœì‹ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  5ê°œë¡œ ì œí•œ
        print(f"ğŸ”¢ Before sorting: {len(articles)} articles")  # ë””ë²„ê¹… ë¡œê·¸
        try:
            articles = sorted(articles, key=get_sort_key, reverse=True)[:5]
            print(f"âœ… After sorting and limiting: {len(articles)} articles")  # ë””ë²„ê¹… ë¡œê·¸
        except Exception as sort_err:
            print(f"âŒ Sorting failed: {sort_err}")  # ë””ë²„ê¹… ë¡œê·¸
            # ì •ë ¬ ì‹¤íŒ¨ì‹œ ê·¸ëƒ¥ ì²˜ìŒ 5ê°œ ì‚¬ìš©
            articles = articles[:5]
        print(f"ğŸ“Š Processing {len(articles)} most recent articles for {category}")  # ë””ë²„ê¹… ë¡œê·¸

        try:

            for i, article in enumerate(articles):
                title = article.get("title", "").strip()
                description = article.get("description", "").strip()

                # HTML íƒœê·¸ ì œê±°ë§Œ í•˜ê³  ë
                if description:
                    soup = BeautifulSoup(description, 'html.parser')
                    description = soup.get_text().strip()
                    description = ' '.join(description.split())

                total_processed += 1
                print(f"ğŸ“° Processing article {i+1}: {title[:50]}...")

                # ê°„ë‹¨í•˜ê²Œ ì €ì¥
                content = description  # RSS ìš”ì•½ì„ ë³¸ë¬¸ìœ¼ë¡œ ì‚¬ìš©
                news_url = article.get("url", "")

                # ì¤‘ë³µ ì²´í¬ ê°„ë‹¨í•˜ê²Œ
                existing = db.query(Post).filter(Post.title == title).first()
                if existing:
                    print(f"ğŸ”„ Skipped: Already exists - {title[:30]}...")
                    continue

                # ì €ì¥
                full_content = content
                if news_url:
                    full_content += f"\n\nğŸ”— ì „ì²´ ê¸°ì‚¬ ë³´ê¸°: {news_url}"

                image_url = article.get("urlToImage", "")
                if not image_url:
                    image_url = "https://images.unsplash.com/photo-1504711434969-e33886168f5c?auto=format&fit=crop&q=80&w=800"

                post_data = {
                    "title": title[:200],
                    "summary": description[:300],
                    "content": full_content,
                    "category": category.capitalize(),
                    "image_url": image_url
                }

                db_post = Post(**post_data)
                db.add(db_post)
                total_saved += 1
                print(f"âœ… Saved article: {title[:30]}...")
                
        except Exception as e:
            print(f"ğŸ’¥ Error fetching {category} news: {e}")
            continue

    try:
        db.commit()
        print(f"ğŸ‰ Total processed: {total_processed}, Total saved: {total_saved}")  # ìµœì¢… ê²°ê³¼ ë¡œê·¸
        print("News fetched and stored successfully")
    except Exception as e:
        db.rollback()
        print(f"ğŸ’¥ Error saving news to database: {e}")

# API ì•¤ë“œ í¬ì¸íŠ¸ë“¤
@app.get("/api/posts", response_model=List[PostResponse])
async def get_posts(
    category: Optional[str] = Query(None),
    search: Optional[str] = Query(None),
    db: Session = Depends(get_db)
):
    query = db.query(Post)

    if category:
        query = query.filter(Post.category == category)

    if search:
        search_term = f"%{search.lower()}%"
        query = query.filter(
            (Post.title.ilike(search_term)) |
            (Post.content.ilike(search_term))
        )

    posts = query.order_by(Post.created_at.desc()).all()
    return posts

# FastAPIì—ì„œëŠ” ê²½ë¡œ íŒŒë¼ë¯¸í„°ë¥¼ ì¤‘ê´„í˜¸ë¡œ ì„ ì–¸í•´ì•¼ í•˜ë©°, f-stringì„ ì‚¬ìš©í•  í•„ìš”ê°€ ì—†ë‹¤.
@app.api_route("/api/posts/{post_id}", methods=["GET"])  # api_routeë¡œ ë³€ê²½í•˜ì—¬ validation ìš°íšŒ
async def get_post(post_id, db: Session = Depends(get_db)):  # íƒ€ì… íŒíŠ¸ ì œê±°
    print(f"DEBUG: Requesting post with ID: {post_id}, type: {type(post_id)}")

    try:
        post_id_int = int(post_id)
        print(f"DEBUG: Converted to int: {post_id_int}")
    except ValueError as e:
        print(f"DEBUG: Failed to convert ID to int: {e}")
        raise HTTPException(status_code=400, detail=f"Invalid ID format: {post_id}")

    # ë°ì´í„°ë² ì´ìŠ¤ì— í•´ë‹¹ IDê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    all_posts = db.query(Post).all()
    print(f"DEBUG: All post IDs in database: {[p.id for p in all_posts]}")

    post = db.query(Post).filter(Post.id == post_id_int).first()
    if not post:
        print(f"DEBUG: Post with ID {post_id_int} not found")
        raise HTTPException(status_code=404, detail="Post not found")

    print(f"DEBUG: Found post: {post.id}, {post.title}")
    return post


@app.post("/api/posts", response_model=PostResponse, status_code=201)
async def create_post(post: PostCreate, db: Session = Depends(get_db)):
    db_post = Post(**post.dict())
    db.add(db_post)
    db.commit()
    db.refresh(db_post)
    return db_post


@app.post("/api/news/fetch")
async def fetch_latest_news(db: Session = Depends(get_db)):
    """ìµœì‹  ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì™€ì„œ ì €ì¥"""
    await fetch_and_store_news(db)
    return {"message": "Latest news fetched and stored successfully"}
    
        

# ê°„ë‹¨í•œ ì„œë²„ ì‹¤í–‰
if __name__ == "__main__":
    port = int(os.getenv("PORT", 8000))
    uvicorn.run(app, host="127.0.0.1", port=port)



