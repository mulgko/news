from fastapi import FastAPI, HTTPException, Query, Depends
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field, field_serializer
from datetime import datetime
from typing import Optional, List, AsyncGenerator, Dict
from contextlib import asynccontextmanager
import os
from dotenv import load_dotenv
from sqlalchemy import create_engine, Column, Integer, String, Text, TIMESTAMP, func
from sqlalchemy.orm import declarative_base
from sqlalchemy.orm import sessionmaker, Session
import uvicorn
import httpx
import feedparser
import requests
from bs4 import BeautifulSoup
import time
import re
import base64
from googlenewsdecoder import gnewsdecoder
from fake_useragent import UserAgent
from urllib.parse import urlparse, parse_qs
from newspaper import Article, Config
import ssl
import urllib3
import certifi

# SSL í™˜ê²½ ì„¤ì • ê°œì„  (ë” ê°•ë ¥í•œ SSL ìš°íšŒ)
os.environ['SSL_CERT_FILE'] = certifi.where()
ssl._create_default_https_context = ssl._create_unverified_context
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ì¶”ê°€ SSL ìš°íšŒ ì„¤ì •
import warnings
warnings.filterwarnings('ignore', message='Unverified HTTPS request')

# requests ì„¸ì…˜ì— SSL ìš°íšŒ ì ìš©
session = requests.Session()
session.verify = False
adapter = requests.adapters.HTTPAdapter(
    pool_connections=10,
    pool_maxsize=10,
    max_retries=3,
    pool_block=False
)
session.mount('http://', adapter)
session.mount('https://', adapter)

def get_sort_key(article):
    """ê¸°ì‚¬ ì •ë ¬ì„ ìœ„í•œ í‚¤ í•¨ìˆ˜ - ìµœì‹ ìˆœ ì •ë ¬"""
    published_date = article.get("publishedAt", "")
    if published_date:
        try:
            # ì´ë¯¸ ISO formatì´ë¯€ë¡œ ë°”ë¡œ íŒŒì‹±
            if published_date.endswith('Z'):
                published_date = published_date.replace('Z', '+00:00')
            return datetime.fromisoformat(published_date)
        except Exception as e:
            print(f"âš ï¸ Date parsing error for article: {article.get('title', '')[:30]}... - {e}")
            return datetime.min
    return datetime.min


def decode_google_news_url(url: str, session=None) -> str:
    """
    Comprehensive Google News URL decoder with multiple fallback strategies.
    Returns the actual article URL from a Google News redirect link.
    """
    if not url or "google.com" not in url:
        return url

    # Preprocessing: URLì—ì„œ ë¯¸êµ­ ë¦¬ë‹¤ì´ë ‰ì…˜ íŒŒë¼ë¯¸í„° ì œê±°
    original_url = url
    url = re.sub(r'&hl=[^&]*&gl=[^&]*&ceid=[^&]*', '', url)
    if url != original_url:
        print(f"ğŸ§¹ Removed US redirect params from URL")

    # Strategy 0: googlenewsdecoder ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¨¼ì € ì‹œë„ (ê°€ì¥ íš¨ê³¼ì !)
    try:
        result = gnewsdecoder(url, interval=1)
        if result.get("status") and result.get("decoded_url"):
            decoded = result["decoded_url"]
            # Google URLì´ ì•„ë‹Œ ì‹¤ì œ ë‰´ìŠ¤ URLì¸ì§€ í™•ì¸
            if "google.com" not in decoded and decoded.startswith('http'):
                print(f"âœ… Decoded by googlenewsdecoder: {decoded[:80]}...")
                return decoded
    except Exception as e:
        print(f"âš ï¸ googlenewsdecoder failed: {e}")

    # Strategy 0.5: URLì—ì„œ ë¯¸êµ­ ë¦¬ë‹¤ì´ë ‰ì…˜ íŒŒë¼ë¯¸í„° ì œê±° í›„ ì¬ì‹œë„
    try:
        # ë¯¸êµ­ ì§€ì—­ íŒŒë¼ë¯¸í„° ì œê±° (&hl=en-US&gl=US&ceid=US:en ë“±)
        clean_url = re.sub(r'&hl=[^&]*&gl=[^&]*&ceid=[^&]*', '', url)
        if clean_url != url:
            print(f"ğŸ§¹ Cleaned US redirect params, retrying...")
            result = gnewsdecoder(clean_url, interval=1)
            if result.get("status") and result.get("decoded_url"):
                decoded = result["decoded_url"]
                if "google.com" not in decoded and decoded.startswith('http'):
                    print(f"âœ… Decoded after cleaning: {decoded[:80]}...")
                    return decoded
    except Exception as e:
        print(f"âš ï¸ URL cleaning failed: {e}")

    # Strategy 1: Extract from URL parameters
    try:
        parsed = urlparse(url)
        if 'url' in parse_qs(parsed.query):
            direct_url = parse_qs(parsed.query)['url'][0]
            if "google.com" not in direct_url:
                print(f"âœ… Extracted from URL params: {direct_url[:80]}...")
                return direct_url
    except Exception as e:
        print(f"âš ï¸ URL param extraction failed: {e}")

    # Strategy 2: Base64 decode from articles path
    if "articles/" in url:
        try:
            # Extract the encoded portion
            encoded_str = url.split("articles/")[1].split("?")[0]

            # Fix padding
            padding = (4 - len(encoded_str) % 4) % 4
            encoded_str += "=" * padding

            # Replace URL-safe characters
            encoded_str = encoded_str.replace('-', '+').replace('_', '/')

            # Decode
            decoded_bytes = base64.b64decode(encoded_str)

            # Method A: Look for http(s):// pattern in binary
            match = re.search(rb'https?://[^\x00-\x20\x7f-\xff]+', decoded_bytes)
            if match:
                real_url = match.group(0).decode('utf-8', errors='ignore')
                # Clean up any trailing garbage
                real_url = re.sub(r'[^\x21-\x7E]+$', '', real_url)

                if "google.com" not in real_url and len(real_url) > 10:
                    print(f"âœ… Base64 decoded (method A): {real_url[:80]}...")
                    return real_url

            # Method B: Try to decode as UTF-8 and extract URL
            try:
                decoded_str = decoded_bytes.decode('utf-8', errors='ignore')
                url_match = re.search(r'https?://[^\s\x00-\x1f\x7f-\xff]+', decoded_str)
                if url_match:
                    real_url = url_match.group(0)
                    if "google.com" not in real_url:
                        print(f"âœ… Base64 decoded (method B): {real_url[:80]}...")
                        return real_url
            except:
                pass

        except Exception as e:
            print(f"âš ï¸ Base64 decoding failed: {e}")

    # Strategy 3: Follow redirects with requests
    if session is None:
        session = requests.Session()

    try:
        # í•œêµ­ ì§€ì—­ í—¤ë”ë¡œ ë¯¸êµ­ ë¦¬ë‹¤ì´ë ‰ì…˜ ë°©ì§€
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',  # í•œêµ­ì–´ ìš°ì„ 
            'Referer': 'https://news.google.com/',
            'Accept-Encoding': 'gzip, deflate, br',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache',
            # í•œêµ­ IP ì‹œë®¬ë ˆì´ì…˜ í—¤ë”
            'X-Forwarded-For': '121.165.0.0',  # í•œêµ­ IP ë²”ìœ„
            'CF-IPCountry': 'KR',  # Cloudflare êµ­ê°€ ì½”ë“œ
            'X-Real-IP': '121.165.0.0',
        }

        # Follow redirects
        response = session.get(url, headers=headers, timeout=10, allow_redirects=True, verify=False)
        final_url = response.url

        # ë¯¸êµ­ ë¦¬ë‹¤ì´ë ‰ì…˜ íŒŒë¼ë¯¸í„°ê°€ ë¶™ì—ˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì œê±°
        final_url = re.sub(r'&hl=[^&]*&gl=[^&]*&ceid=[^&]*', '', final_url)

        # Check if we escaped Google News
        if "google.com" not in final_url:
            print(f"âœ… Redirect followed: {final_url[:80]}...")
            return final_url

        # Strategy 4: Parse HTML for the actual link
        soup = BeautifulSoup(response.text, 'html.parser')

        # Look for meta refresh
        meta_refresh = soup.find('meta', attrs={'http-equiv': 'refresh'})
        if meta_refresh:
            content = meta_refresh.get('content', '')
            if 'url=' in content.lower():
                redirect_url = content.lower().split('url=')[-1].strip()
                if "google.com" not in redirect_url:
                    print(f"âœ… Meta refresh found: {redirect_url[:80]}...")
                    return redirect_url

        # Look for links that aren't Google
        for link in soup.find_all('a', href=True):
            href = link['href']
            if href.startswith('http') and "google.com" not in href and len(href) > 15:
                print(f"âœ… Link extracted from HTML: {href[:80]}...")
                return href

    except Exception as e:
        print(f"âš ï¸ Redirect following failed: {e}")

    # Strategy 5: Try direct GET without redirects to parse location header
    try:
        response = session.get(url, headers=headers, timeout=10, allow_redirects=False, verify=False)
        if 'Location' in response.headers:
            location = response.headers['Location']
            if "google.com" not in location:
                print(f"âœ… Location header: {location[:80]}...")
                return location
    except Exception as e:
        print(f"âš ï¸ Location header check failed: {e}")

    print(f"âŒ All decoding strategies failed for: {url[:80]}...")
    return url


def extract_news_content(article_url: str, session=None) -> str:
    """
    Extract full article content from a news URL.
    First decodes Google News URLs, then extracts content.
    """
    if session is None:
        session = requests.Session()
        session.verify = False

    try:
        # Step 1: Decode Google News URL
        real_url = decode_google_news_url(article_url, session)

        # If still a Google News URL, abort
        if "news.google.com" in real_url:
            print(f"âŒ Could not decode Google News URL")
            return None

        print(f"ğŸ“„ Fetching content from: {real_url[:80]}...")

        # Step 2: Fetch the actual article
        time.sleep(1)  # Be polite

        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        }

        response = session.get(real_url, headers=headers, timeout=15, verify=False)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'html.parser')

        # Remove unwanted elements
        for element in soup.find_all(['script', 'style', 'nav', 'footer', 'header', 'aside']):
            element.decompose()

        # Try multiple content selectors (Korean news sites)
        content_selectors = [
            'article',
            '[id*="article"]',
            '[class*="article"]',
            '[id*="content"]',
            '[class*="content"]',
            '#articleBody',
            '#newsct_article',
            '.article_body',
            '.news_body',
            'div[itemprop="articleBody"]',
            '.article-content',
            'main'
        ]

        content_text = ""
        for selector in content_selectors:
            elements = soup.select(selector)
            if elements:
                # Get text from all matching elements
                texts = []
                for elem in elements:
                    # Get all paragraphs
                    paragraphs = elem.find_all(['p', 'div'])
                    for p in paragraphs:
                        text = p.get_text(strip=True)
                        if len(text) > 30:  # Only substantial paragraphs
                            texts.append(text)

                if texts:
                    content_text = '\n\n'.join(texts)
                    break

        # Fallback: get all paragraphs
        if len(content_text) < 200:
            paragraphs = soup.find_all('p')
            texts = [p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 30]
            content_text = '\n\n'.join(texts)

        # Clean up the text
        if content_text:
            # Remove common Korean news site artifacts
            content_text = re.sub(r'â–¶.*?\n', '', content_text)
            content_text = re.sub(r'\[.*?\]', '', content_text)
            content_text = re.sub(r'ì‚¬ì§„.*?\n', '', content_text)
            content_text = re.sub(r'\s+', ' ', content_text)
            content_text = content_text.strip()

        if len(content_text) > 100:
            print(f"âœ… Extracted {len(content_text)} characters")
            return content_text[:2000]  # Limit length

        print(f"âŒ Content too short: {len(content_text)} characters")
        return None

    except Exception as e:
        print(f"âŒ Content extraction error: {e}")
        return None




# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


# ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
DATABASE_URL = os.getenv("DATABASE_URL") or "sqlite:///./news.db"

engine = create_engine(DATABASE_URL, echo=True)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

# ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë¸
class Post(Base):
    __tablename__ = "posts"

    id = Column(Integer, primary_key=True, index=True)
    title = Column(String, nullable=False)
    summary = Column(Text, nullable=False)
    content = Column(Text, nullable=False)
    category = Column(String, nullable=False)
    image_url = Column("image_url", String, nullable=False)
    created_at = Column("created_at", TIMESTAMP, server_default=func.now())

# Pydantic ìŠ¤í‚¤ë§ˆ
class PostBase(BaseModel):
    title: str
    summary: str
    content: str
    category: str
    image_url: str

class PostCreate(PostBase):
    pass

class PostResponse(PostBase):
    id: int
    created_at: Optional[datetime] = None

    @field_serializer('created_at')
    def serialize_created_at(self, value: Optional[datetime]) -> Optional[str]:
        if value is None:
            return None
        return value.isoformat()

    model_config = {
        "from_attributes": True,
        "populate_by_name": True
    }


# lifespan ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
@asynccontextmanager
async def lifespan(app: FastAPI) -> AsyncGenerator[None, None]:
    # startup
    Base.metadata.create_all(bind=engine)
    await seed_database()
    yield
    # shutdown (í•„ìš”ì‹œ cleanup ì½”ë“œ ì¶”ê°€)


# FastAPI ì•± ìƒì„±
app = FastAPI(title="News API", version="1.0.0", lifespan=lifespan)

# CORS ì„¤ì • (í•„ìš”ì‹œ)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://127.0.0.1:5173"],  # Vite ê¸°ë³¸ í¬íŠ¸
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ ì˜ì¡´ì„±
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


# ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” (í…Œì´ë¸” ìƒì„±)



async def seed_database():
    db = SessionLocal()
    try:
        # ê¸°ì¡´ ë°ì´í„° ëª¨ë‘ ì‚­ì œ (ìŠ¤í‚¤ë§ˆ ë³€ê²½ìœ¼ë¡œ ì¸í•œ ë¦¬ì…‹)
        db.query(Post).delete()
        db.commit()
        print("Existing posts deleted for schema update")
        # ì‹œë“œ ë°ì´í„°
        seed_posts = [
            {
                "title": "2025ë…„ AIì˜ ë¯¸ë˜ ì „ë§",
                "summary": "ì¸ê³µì§€ëŠ¥ì´ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë‚´ë…„ì— ì–´ë–¤ ë³€í™”ê°€ ì˜ˆìƒë˜ëŠ”ì§€ ì•Œì•„ë³´ì„¸ìš”.",
                "content": "ì¸ê³µì§€ëŠ¥ì´ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë‚´ë…„ì— ì–´ë–¤ ë³€í™”ê°€ ì˜ˆìƒë˜ëŠ”ì§€ ì•Œì•„ë³´ì„¸ìš”. ì „ë¬¸ê°€ë“¤ì€ ìƒì„±í˜• ëª¨ë¸ê³¼ ììœ¨ ì—ì´ì „íŠ¸ ë¶„ì•¼ì—ì„œ ì£¼ìš” ëŒíŒŒêµ¬ë¥¼ ì˜ˆìƒí•˜ê³  ìˆìŠµë‹ˆë‹¤. AIì˜ ì¼ìƒìƒí™œ í†µí•©ì´ ë”ìš± ì›í™œí•´ì§€ë©° ì˜ë£Œ, ê¸ˆìœµ ë“± ë‹¤ì–‘í•œ ì‚°ì—…ì— ì˜í–¥ì„ ë¯¸ì¹  ê²ƒì…ë‹ˆë‹¤.",
                "category": "ê¸°ìˆ ",
                "image_url": "https://images.unsplash.com/photo-1677442136019-21780ecad995?auto=format&fit=crop&q=80&w=800",
            },
            {
                "title": "ì¸í”Œë ˆì´ì…˜ ì™„í™”ë¡œ ê¸€ë¡œë²Œ ì¦ì‹œ ìƒìŠ¹",
                "summary": "ì´ë²ˆ ì£¼ ê²½ì œ ì§€í‘œ í˜¸ì¡°ë¡œ ì£¼ì‹ ì‹œì¥ì´ ì‹ ê³ ì ì„ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤.",
                "content": "ì´ë²ˆ ì£¼ ê²½ì œ ì§€í‘œ í˜¸ì¡°ë¡œ ì£¼ì‹ ì‹œì¥ì´ ì‹ ê³ ì ì„ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤. íˆ¬ììë“¤ì€ ì¤‘ì•™ì€í–‰ì˜ ë‹¤ìŒ ì›€ì§ì„ì— ëŒ€í•´ ë‚™ê´€ì ì…ë‹ˆë‹¤. ê¸°ìˆ ì£¼ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì£¼ìš” ì§€ìˆ˜ê°€ ì‚¬ìƒ ìµœê³ ì¹˜ë¥¼ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤.",
                "category": "ë¹„ì¦ˆë‹ˆìŠ¤",
                "image_url": "https://images.unsplash.com/photo-1611974765270-ca12586343bb?auto=format&fit=crop&q=80&w=800",
            },
            {
                "title": "ìƒëª…ì¡´ì— ìœ„ì¹˜í•œ ìƒˆë¡œìš´ í–‰ì„± ë°œê²¬",
                "summary": "ì²œë¬¸í•™ìë“¤ì´ ì§€êµ¬ì™€ ìœ ì‚¬í•œ ì ì¬ì  í–‰ì„±ì„ 40ê´‘ë…„ ê±°ë¦¬ì—ì„œ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.",
                "content": "ì²œë¬¸í•™ìë“¤ì´ ì§€êµ¬ì™€ ìœ ì‚¬í•œ ì ì¬ì  í–‰ì„±ì„ 40ê´‘ë…„ ê±°ë¦¬ì—ì„œ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ê¸€ë¦¬ì œ 12 bë¡œ ëª…ëª…ëœ ì´ í–‰ì„±ì€ ì ìƒ‰ ì™œì„± ì£¼ìœ„ë¥¼ ê³µì „í•˜ë©° ì•¡ì²´ ë¬¼ì„ ìœ ì§€í•  ìˆ˜ ìˆëŠ” ì˜¨ë„ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì œì„ìŠ¤ ì›¹ ìš°ì£¼ ë§ì›ê²½ì„ í†µí•œ ì¶”ê°€ ê´€ì¸¡ì´ ê³„íšë˜ì–´ ìˆìŠµë‹ˆë‹¤.",
                "category": "ê³¼í•™",
                "image_url": "https://images.unsplash.com/photo-1451187580459-43490279c0fa?auto=format&fit=crop&q=80&w=800",
            },
            {
                "title": "ì˜¤ëŠ˜ ë°¤ ìˆ™ë©´ì„ ìœ„í•œ 5ê°€ì§€ íŒ",
                "summary": "ìˆ™ë©´ì„ ì·¨í•˜ê¸° ì–´ë ¤ìš°ì‹ ê°€ìš”? ê³¼í•™ì ìœ¼ë¡œ ê²€ì¦ëœ íŒì„ í™•ì¸í•˜ì„¸ìš”.",
                "content": "ìˆ™ë©´ì„ ì·¨í•˜ê¸° ì–´ë ¤ìš°ì‹ ê°€ìš”? ê³¼í•™ì ìœ¼ë¡œ ê²€ì¦ëœ íŒì„ í™•ì¸í•˜ì„¸ìš”. 1. ê·œì¹™ì ì¸ ì¼ì • ìœ ì§€í•˜ê¸°. 2. í¸ì•ˆí•œ í™˜ê²½ ì¡°ì„±í•˜ê¸°. 3. ì·¨ì¹¨ ì „ í™”ë©´ ì‹œê°„ ì œí•œí•˜ê¸°. 4. ë¨¹ëŠ” ìŒì‹ê³¼ ë§ˆì‹œëŠ” ìŒë£Œ ì£¼ì˜í•˜ê¸°. 5. ì¼ìƒ ìƒí™œì— ì‹ ì²´ í™œë™ í¬í•¨í•˜ê¸°.",
                "category": "ê±´ê°•",
                "image_url": "https://images.unsplash.com/photo-1541781777621-794453259724?auto=format&fit=crop&q=80&w=800",
            },
            {
                "title": "ì˜¬ì—¬ë¦„ ë³¼ë§Œí•œ ê¸°ëŒ€ì‘ ì˜í™”ë“¤",
                "summary": "íŒì½˜ ì¤€ë¹„í•˜ì„¸ìš”! ì´ë²ˆ ì‹œì¦Œ ê°€ì¥ ê¸°ëŒ€ë˜ëŠ” ì˜í™”ë“¤ì„ ì†Œê°œí•©ë‹ˆë‹¤.",
                "content": "íŒì½˜ ì¤€ë¹„í•˜ì„¸ìš”! ì´ë²ˆ ì‹œì¦Œ ê°€ì¥ ê¸°ëŒ€ë˜ëŠ” ì˜í™”ë“¤ì„ ì†Œê°œí•©ë‹ˆë‹¤. ìŠˆí¼íˆì–´ë¡œ ëŒ€ì‘ë¶€í„° ë”°ëœ»í•œ ê°ë™ ì• ë‹ˆë©”ì´ì…˜ê¹Œì§€ ëª¨ë‘ë¥¼ ìœ„í•œ ì‘í’ˆì´ ì¤€ë¹„ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ë²ˆ ì—¬ë¦„ ê·¹ì¥ì—ì„œ ë³¼ ìˆ˜ ìˆëŠ” í•„ëŒ ì˜í™” ëª©ë¡ì„ í™•ì¸í•´ë³´ì„¸ìš”.",
                "category": "ì—”í„°í…Œì¸ë¨¼íŠ¸",
                "image_url": "https://images.unsplash.com/photo-1536440136628-849c177e76a1?auto=format&fit=crop&q=80&w=800",
            },
        ]

        for post_data in seed_posts:
            post = Post(**post_data)
            db.add(post)
        db.commit()
        print("Database seeded with initial posts")
    except Exception as e: 
        db.rollback()
        print(f"Error seeding database: {e}")
    finally:
        db.close()



# Google News RSS Client
class GoogleNewsRSSClient:
    def __init__(self):
        # í•œêµ­ ë‰´ìŠ¤ RSS í”¼ë“œ
        self.base_url = "https://news.google.com/rss"
        # ê¸€ë¡œë²Œ SSL ìš°íšŒ ì„¸ì…˜ ì‚¬ìš© + í•œêµ­ ì§€ì—­ í—¤ë” ì„¤ì •
        self.session = session
        self.session.headers.update({
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'https://news.google.com/',
            'X-Forwarded-For': '121.165.0.0',  # í•œêµ­ IP ì‹œë®¬ë ˆì´ì…˜
            'CF-IPCountry': 'KR',
        })

    def extract_article_content(self, url: str) -> str:
        """ë‰´ìŠ¤ ê¸°ì‚¬ URLì—ì„œ ì „ì²´ ë‚´ìš©ì„ ì¶”ì¶œ"""
        try:
            # ìš”ì²­ ê°„ ë”œë ˆì´ ì¶”ê°€ (í¬ë¡¤ë§ ì˜ˆì˜)
            time.sleep(1)

            response = self.session.get(url, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')

            # ë‹¤ì–‘í•œ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì˜ ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„
            content_selectors = [
                'article',  # ì¼ë°˜ì ì¸ article íƒœê·¸
                '[class*="content"]',  # content í´ë˜ìŠ¤ê°€ í¬í•¨ëœ ìš”ì†Œ
                '[class*="article"]',  # article í´ë˜ìŠ¤ê°€ í¬í•¨ëœ ìš”ì†Œ
                '[class*="story"]',  # story í´ë˜ìŠ¤ê°€ í¬í•¨ëœ ìš”ì†Œ
                'div[itemprop="articleBody"]',  # schema.org ë§ˆí¬ì—…
                '.news-content',  # ë„¤ì´ë²„ ë‰´ìŠ¤
                '#articleBodyContents',  # ë‹¤ìŒ ë‰´ìŠ¤
                '.article-body',  # ì¼ë°˜ì ì¸ ë³¸ë¬¸ í´ë˜ìŠ¤
                'p'  # ëª¨ë“  p íƒœê·¸ (fallback)
            ]

            for selector in content_selectors:
                content_elements = soup.select(selector)
                if content_elements:
                    # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ë¦¬
                    content_text = ' '.join([elem.get_text().strip() for elem in content_elements if elem.get_text().strip()])

                    # ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±° (ê´‘ê³ , ê´€ë ¨ ê¸°ì‚¬ ë“±)
                    content_text = re.sub(r'â–¶.*?\n', '', content_text)  # ë„¤ì´ë²„ ë‰´ìŠ¤ í™”ì‚´í‘œ ì œê±°
                    content_text = re.sub(r'\[.*?\]', '', content_text)  # ëŒ€ê´„í˜¸ ì•ˆ í…ìŠ¤íŠ¸ ì œê±°
                    content_text = re.sub(r'ì‚¬ì§„.*?\n', '', content_text)  # ì‚¬ì§„ ì„¤ëª… ì œê±°
                    content_text = re.sub(r'\s+', ' ', content_text)  # ì—°ì†ëœ ê³µë°± ì œê±°

                    if len(content_text) > 100:  # ì¶©ë¶„í•œ ê¸¸ì´ì˜ ë‚´ìš©ì¸ì§€ í™•ì¸
                        return content_text[:2000]  # ê¸¸ì´ ì œí•œ

            return ""  # ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨

        except Exception as e:
            print(f"Error extracting content from {url}: {e}")
            return ""

    def _extract_real_url(self, google_news_url: str) -> str:
        """Google News URLì—ì„œ ì‹¤ì œ ë‰´ìŠ¤ URL ì¶”ì¶œ (ê°„ì†Œí™”ëœ ë²„ì „)"""
        # ìƒˆë¡œ ë§Œë“  ì „ë¬¸ ë””ì½”ë” ì‚¬ìš© - self.session ì „ë‹¬!
        return decode_google_news_url(google_news_url, self.session)

    def get_news_by_topic(self, topic: str = "general") -> List[Dict]:
        """Google News ê²€ìƒ‰ RSSì—ì„œ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
        # í† í”½ë³„ ê²€ìƒ‰ì–´ ë§¤í•‘ (ë” ì•ˆì •ì ì¸ ë°©ì‹)
        topic_queries = {
            "business": "ë¹„ì¦ˆë‹ˆìŠ¤ OR ê²½ì œ OR ê¸°ì—… OR ê¸ˆìœµ",
            "technology": "ê¸°ìˆ  OR IT OR ì¸ê³µì§€ëŠ¥ OR ìŠ¤íƒ€íŠ¸ì—…",
            "science": "ê³¼í•™ OR ì—°êµ¬ OR ìš°ì£¼ OR í™˜ê²½",
            "health": "ê±´ê°• OR ì˜ë£Œ OR ë³‘ì› OR ì½”ë¡œë‚˜",
            "entertainment": "ì—°ì˜ˆ OR ì˜í™” OR ìŒì•… OR ë“œë¼ë§ˆ",
            "general": ""
        }

        # Google News ê²€ìƒ‰ RSS URL ìƒì„±
        base_url = "https://news.google.com/rss/search?q="
        query = topic_queries.get(topic, "")
        if query:
            # ê²€ìƒ‰ì–´ë¥¼ URL ì¸ì½”ë”©
            import urllib.parse
            encoded_query = urllib.parse.quote(query)
            rss_url = f"{base_url}{encoded_query}&hl=ko&gl=KR&ceid=KR:ko"
        else:
            rss_url = "https://news.google.com/rss?hl=ko&gl=KR&ceid=KR:ko"

        try:
            # RSS í”¼ë“œ íŒŒì‹±
            print(f"ğŸŒ Fetching RSS from: {rss_url}")  # ë””ë²„ê¹… ë¡œê·¸

            # SSL ê²€ì¦ ì—†ì´ RSS ê°€ì ¸ì˜¤ê¸° (requests ì‚¬ìš©)
            response = self.session.get(rss_url, verify=False)
            response.raise_for_status()
            rss_content = response.text

            # ê°€ì ¸ì˜¨ RSS í…ìŠ¤íŠ¸ë¥¼ feedparserë¡œ íŒŒì‹±
            feed = feedparser.parse(rss_content)

            # ìƒì„¸í•œ ë””ë²„ê¹… ì •ë³´
            print(f"ğŸ“¡ Feed status: {feed.status if hasattr(feed, 'status') else 'unknown'}")
            print(f"ğŸ“° Feed entries count: {len(feed.entries)}")
            print(f"ğŸ“ Feed title: {getattr(feed.feed, 'title', 'No title')}")
            print(f"ğŸ” Feed keys: {list(feed.keys())}")
            print(f"ğŸ“„ Raw feed data (first 500 chars): {str(feed)[:500]}")

            if hasattr(feed, 'bozo') and feed.bozo:
                print(f"âš ï¸ Feed parsing error: {feed.bozo_exception}")

            # entries ìƒì„¸ ì •ë³´
            if feed.entries:
                print(f"âœ… First entry keys: {list(feed.entries[0].keys()) if feed.entries else 'No entries'}")
                print(f"âœ… First entry title: {getattr(feed.entries[0], 'title', 'No title') if feed.entries else 'No entries'}")
            else:
                print(f"âŒ No entries found in feed")

            articles = []
            for entry in feed.entries[:20]:  # ìµœëŒ€ 20ê°œ ë‰´ìŠ¤
                # ì´ë¯¸ì§€ URL ì¶”ì¶œ ê°œì„ 
                image_url = ""
                if hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
                    image_url = entry.media_thumbnail[0].get('url', '')
                elif hasattr(entry, 'media_content') and entry.media_content:
                    image_url = entry.media_content[0].get('url', '')
                elif hasattr(entry, 'enclosures') and entry.enclosures:
                    for enclosure in entry.enclosures:
                        if enclosure.get('type', '').startswith('image/'):
                            image_url = enclosure.get('url', '')
                            break

                # ë‚ ì§œ ì²˜ë¦¬ ê°œì„ 
                published_at = getattr(entry, 'published', '')
                if published_at:
                    try:
                        from email.utils import parsedate_to_datetime
                        published_at = parsedate_to_datetime(published_at).isoformat()
                    except:
                        published_at = datetime.now().isoformat()

                # Google News ë§í¬ì—ì„œ ì‹¤ì œ ë‰´ìŠ¤ URL ì¶”ì¶œ ì‹œë„
                actual_url = self._extract_real_url(entry.link)

                article = {
                    "title": entry.title,
                    "description": getattr(entry, 'summary', ''),
                    "content": getattr(entry, 'summary', ''),  # RSSì—ì„œëŠ” ì½˜í…ì¸ ê°€ ì œí•œì 
                    "url": actual_url,  # ì‹¤ì œ ë‰´ìŠ¤ URL ì‚¬ìš©
                    "urlToImage": image_url,
                    "publishedAt": published_at
                }
                articles.append(article)

            print(f"âœ… Returning {len(articles)} articles")
            return articles

        except Exception as e:
            print(f"ğŸ’¥ Error parsing RSS feed for {topic}: {e}")
            import traceback
            print(f"ğŸ’¥ Full traceback: {traceback.format_exc()}")
            return []
        







# News fetch, save Func
async def fetch_and_store_news(db: Session):
    """Google News RSSì—ì„œ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì™€ì„œ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
    client = GoogleNewsRSSClient()

    # ì—¬ëŸ¬ ì¹´í…Œê³ ë¦¬ì˜ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
    categories = ["business", "technology", "science", "health", "entertainment"]

    total_processed = 0
    total_saved = 0

    # User-Agent ë‹¤ì–‘í™”ë¥¼ ìœ„í•œ ê°ì²´ ìƒì„±
    ua = UserAgent()

    for category in categories:
        # ì¹´í…Œê³ ë¦¬ë³„ë¡œ ìƒˆë¡œìš´ ì„¸ì…˜ ìƒì„± (ë” ìì—°ìŠ¤ëŸ¬ìš´ ì ‘ê·¼)
        category_session = requests.Session()
        category_session.verify = False  # SSL ê²€ì¦ ìš°íšŒ
        category_session.headers.update({
            'User-Agent': ua.random,
            'Referer': 'https://news.google.com/',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        print(f"ğŸ” Fetching {category} news...")  # ë””ë²„ê¹… ë¡œê·¸
        articles = client.get_news_by_topic(topic=category)
        print(f"ğŸ“Š Found {len(articles)} articles for {category}")  # ë””ë²„ê¹… ë¡œê·¸

        # ìµœì‹ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  5ê°œë¡œ ì œí•œ
        print(f"ğŸ”¢ Before sorting: {len(articles)} articles")  # ë””ë²„ê¹… ë¡œê·¸
        try:
            articles = sorted(articles, key=get_sort_key, reverse=True)[:5]
            print(f"âœ… After sorting and limiting: {len(articles)} articles")  # ë””ë²„ê¹… ë¡œê·¸
        except Exception as sort_err:
            print(f"âŒ Sorting failed: {sort_err}")  # ë””ë²„ê¹… ë¡œê·¸
            # ì •ë ¬ ì‹¤íŒ¨ì‹œ ê·¸ëƒ¥ ì²˜ìŒ 5ê°œ ì‚¬ìš©
            articles = articles[:5]
        print(f"ğŸ“Š Processing {len(articles)} most recent articles for {category}")  # ë””ë²„ê¹… ë¡œê·¸

        try:

            for i, article in enumerate(articles):
                # ë´‡ ê°ì§€ ë°©ì§€ë¥¼ ìœ„í•œ ë”œë ˆì´ (ì„¸ì…˜ì€ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ìƒˆë¡œ ìƒì„±í•˜ë¯€ë¡œ ì¿ í‚¤ í´ë¦¬ì–´ ë¶ˆí•„ìš”)
                time.sleep(2)
                title = article.get("title", "").strip()
                description = article.get("description", "").strip()

                # HTML íƒœê·¸ ì œê±° (Google News RSSëŠ” HTML í˜•ì‹ì˜ descriptionì„ ì œê³µ)
                if description:
                    soup = BeautifulSoup(description, 'html.parser')
                    # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ê³  ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
                    description = soup.get_text().strip()
                    # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ í†µí•©
                    description = ' '.join(description.split())

                total_processed += 1
                print(f"ğŸ“° Processing article {i+1}: {title[:50]}...")  # ë””ë²„ê¹… ë¡œê·¸
                print(f"ğŸ“ Description after cleaning: {description[:100]}...")  # ë””ë²„ê¹… ë¡œê·¸

                # ì‹¤ì œ ë‰´ìŠ¤ ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„
                news_url = ""
                full_content = None

                # Google Newsì—ì„œ ì‹¤ì œ ë‰´ìŠ¤ URL ì¶”ì¶œ (GoogleNewsRSSClientì—ì„œ "url" í•„ë“œì— ì €ì¥ë¨)
                news_url = article.get("url", "")
                print(f"ğŸ”— News URL: {news_url}")  # ë””ë²„ê¹… ë¡œê·¸

                # ì‹¤ì œ ë‰´ìŠ¤ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ
                if news_url:
                    full_content = extract_news_content(news_url, category_session)

                # content ì„¤ì • (ì‹¤ì œ ë³¸ë¬¸ ìš°ì„ , ì—†ìœ¼ë©´ description ì‚¬ìš©)
                if full_content:
                    content = full_content
                    print(f"ğŸ“„ Using full article content ({len(content)} chars)")  # ë””ë²„ê¹… ë¡œê·¸
                else:
                    content = description
                    print(f"ğŸ“„ Using RSS description ({len(content)} chars)")  # ë””ë²„ê¹… ë¡œê·¸

                # ìœ íš¨ì„± ê²€ì¦
                if not title or not content:
                    print(f"âŒ Skipped: Empty title or content")  # ë””ë²„ê¹… ë¡œê·¸
                    continue
                
                # ì¤‘ë³µ ì²´í¬
                published_date = article.get("publishedAt", "")
                existing = None
                
                if published_date:
                    try:
                        from datetime import datetime
                        pub_dt = datetime.fromisoformat(published_date.replace('Z', '+00:00'))
                        date_str = pub_dt.date().isoformat()
                        existing = db.query(Post).filter(
                            Post.title == title,
                            func.date(Post.created_at) == date_str
                        ).first()
                    except Exception as date_err:
                        print(f"âš ï¸ Date parsing error: {date_err}")  # ë””ë²„ê¹… ë¡œê·¸
                        existing = db.query(Post).filter(Post.title == title).first()
                else:
                    existing = db.query(Post).filter(Post.title == title).first()
                
                if existing:
                    print(f"ğŸ”„ Skipped: Already exists - {title[:30]}...")  # ë””ë²„ê¹… ë¡œê·¸
                    continue
                
                # ì €ì¥í•  ë°ì´í„° ì¤€ë¹„
                # news_urlì€ ìœ„ì—ì„œ ì´ë¯¸ ì¶”ì¶œë¨

                print(f"ğŸ“ Original content length: {len(content)}")  # ë””ë²„ê¹… ë¡œê·¸
                print(f"ğŸ”— News URL: {news_url}")  # ë””ë²„ê¹… ë¡œê·¸

                # RSS ë‚´ìš©ì´ ë¶€ì¡±í•˜ê±°ë‚˜ ë§í¬ë§Œ ìˆìœ¼ë©´ ì‹¤ì œ ê¸°ì‚¬ì—ì„œ ì „ì²´ ë‚´ìš© ì¶”ì¶œ ì‹œë„
                should_extract = (
                    len(content) < 300 or      # ë‚´ìš©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜
                    "http" in content or        # ë§í¬ê°€ í¬í•¨ë˜ì–´ ìˆê±°ë‚˜
                    "..." in content or         # ìƒëµ ê¸°í˜¸ê°€ ìˆê±°ë‚˜
                    content.strip() == "" or    # ë‚´ìš©ì´ ë¹„ì–´ìˆê±°ë‚˜
                    len(content.split()) < 10   # ë‹¨ì–´ê°€ 10ê°œ ë¯¸ë§Œ
                )

                if should_extract and news_url:
                    print(f"ğŸ› ï¸ Extracting full content from: {news_url}")  # ë””ë²„ê¹… ë¡œê·¸
                    try:
                        full_article_content = extract_news_content(news_url, category_session)
                        if full_article_content and len(full_article_content) > len(content):
                            content = full_article_content
                            print(f"âœ… Successfully extracted content ({len(content)} chars)")  # ë””ë²„ê¹… ë¡œê·¸
                        else:
                            print(f"âŒ Failed to extract content or content too short")  # ë””ë²„ê¹… ë¡œê·¸
                            # ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ì›ë³¸ contentë¼ë„ ì‚¬ìš© (ë§í¬ ì œê±°)
                            if "http" in content:
                                content = content.split("http")[0].strip()
                                print(f"ğŸ”§ Cleaned content: {content[:100]}...")  # ë””ë²„ê¹… ë¡œê·¸
                    except Exception as extract_err:
                        print(f"âš ï¸ Content extraction failed: {extract_err}")  # ë””ë²„ê¹… ë¡œê·¸
                        # ì—ëŸ¬ ì‹œì—ë„ ë§í¬ ì œê±°ëœ content ì‚¬ìš©
                        if "http" in content:
                            content = content.split("http")[0].strip()
                
                if not content:
                    print(f"âŒ Skipped: No content available")  # ë””ë²„ê¹… ë¡œê·¸
                    continue
                
                full_content = content
                if news_url:
                    full_content += f"\n\nğŸ”— ì „ì²´ ê¸°ì‚¬ ë³´ê¸°: {news_url}"
                
                image_url = article.get("urlToImage", "")
                if not image_url:
                    image_url = "https://images.unsplash.com/photo-1504711434969-e33886168f5c?auto=format&fit=crop&q=80&w=800"
                
                post_data = {
                    "title": title[:200],
                    "summary": description[:300],
                    "content": full_content,
                    "category": category.capitalize(),
                    "image_url": image_url
                }
                
                try:
                    db_post = Post(**post_data)
                    db.add(db_post)
                    total_saved += 1
                    print(f"âœ… Saved article: {title[:30]}...")  # ë””ë²„ê¹… ë¡œê·¸
                except Exception as save_err:
                    print(f"âŒ Save failed: {save_err}")  # ë””ë²„ê¹… ë¡œê·¸
                    continue
                
        except Exception as e:
            print(f"ğŸ’¥ Error fetching {category} news: {e}")
            continue

    try:
        db.commit()
        print(f"ğŸ‰ Total processed: {total_processed}, Total saved: {total_saved}")  # ìµœì¢… ê²°ê³¼ ë¡œê·¸
        print("News fetched and stored successfully")
    except Exception as e:
        db.rollback()
        print(f"ğŸ’¥ Error saving news to database: {e}")

# API ì•¤ë“œ í¬ì¸íŠ¸ë“¤
@app.get("/api/posts", response_model=List[PostResponse])
async def get_posts(
    category: Optional[str] = Query(None),
    search: Optional[str] = Query(None),
    db: Session = Depends(get_db)
):
    query = db.query(Post)

    if category:
        query = query.filter(Post.category == category)

    if search:
        search_term = f"%{search.lower()}%"
        query = query.filter(
            (Post.title.ilike(search_term)) |
            (Post.content.ilike(search_term))
        )

    posts = query.order_by(Post.created_at.desc()).all()
    return posts

# FastAPIì—ì„œëŠ” ê²½ë¡œ íŒŒë¼ë¯¸í„°ë¥¼ ì¤‘ê´„í˜¸ë¡œ ì„ ì–¸í•´ì•¼ í•˜ë©°, f-stringì„ ì‚¬ìš©í•  í•„ìš”ê°€ ì—†ë‹¤.
@app.api_route("/api/posts/{post_id}", methods=["GET"])  # api_routeë¡œ ë³€ê²½í•˜ì—¬ validation ìš°íšŒ
async def get_post(post_id, db: Session = Depends(get_db)):  # íƒ€ì… íŒíŠ¸ ì œê±°
    print(f"DEBUG: Requesting post with ID: {post_id}, type: {type(post_id)}")

    try:
        post_id_int = int(post_id)
        print(f"DEBUG: Converted to int: {post_id_int}")
    except ValueError as e:
        print(f"DEBUG: Failed to convert ID to int: {e}")
        raise HTTPException(status_code=400, detail=f"Invalid ID format: {post_id}")

    # ë°ì´í„°ë² ì´ìŠ¤ì— í•´ë‹¹ IDê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    all_posts = db.query(Post).all()
    print(f"DEBUG: All post IDs in database: {[p.id for p in all_posts]}")

    post = db.query(Post).filter(Post.id == post_id_int).first()
    if not post:
        print(f"DEBUG: Post with ID {post_id_int} not found")
        raise HTTPException(status_code=404, detail="Post not found")

    print(f"DEBUG: Found post: {post.id}, {post.title}")
    return post


@app.post("/api/posts", response_model=PostResponse, status_code=201)
async def create_post(post: PostCreate, db: Session = Depends(get_db)):
    db_post = Post(**post.dict())
    db.add(db_post)
    db.commit()
    db.refresh(db_post)
    return db_post


@app.post("/api/news/fetch")
async def fetch_latest_news(db: Session = Depends(get_db)):
    """ìµœì‹  ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì™€ì„œ ì €ì¥"""
    await fetch_and_store_news(db)
    return {"message": "Latest news fetched and stored successfully"}
    
        

# Test the decoder
if __name__ == "__main__":
    test_urls = [
        "https://news.google.com/rss/articles/CBMiXkFVX3lxTFA0NWw4ZFpfeEtCS3pZLWl0R19neGE2b2ZaMWxQeGRlSHJNREJYNERWZXcyS0t4blNnODZOMzdOOHYzUWJVNUhqUmdUWmpLSnRPWTFpR2l5NHRpbEh2SEE?oc=5",
        "https://news.google.com/rss/articles/CBMiT0FVX3lxTFBhNmY4UVlHaVNkbEdabDhDUnlfaU1BX3lBTGtXSk5taE1SendEdjRBM0VUVFpBUlV0WlZOWUx6d2dMMkFnd1V3VU1nSHdRV2s?oc=5"
    ]

    session = requests.Session()
    session.verify = False

    for url in test_urls:
        print(f"\n{'='*80}")
        print(f"Testing URL: {url[:80]}...")
        print(f"{'='*80}")
        decoded = decode_google_news_url(url, session)
        print(f"Result: {decoded}")
        print()

    # ì‹¤ì œ ì„œë²„ ì‹¤í–‰
    port = int(os.getenv("PORT", 8000))  # 5000 ëŒ€ì‹  8000 ì‚¬ìš©
    uvicorn.run(app, host="127.0.0.1", port=port)



