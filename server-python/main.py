from fastapi import FastAPI, HTTPException, Query, Depends
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field, field_serializer
from datetime import datetime
from typing import Optional, List, AsyncGenerator, Dict
from contextlib import asynccontextmanager
import os
from dotenv import load_dotenv
from sqlalchemy import create_engine, Column, Integer, String, Text, TIMESTAMP, func
from sqlalchemy.orm import declarative_base
from sqlalchemy.orm import sessionmaker, Session
import uvicorn
import httpx
import feedparser
import requests
from bs4 import BeautifulSoup
import time
import re
import base64
import urllib.parse
from newspaper import Article, Config
import ssl
import urllib3
import certifi

# SSL í™˜ê²½ ì„¤ì • ê°œì„ 
os.environ['SSL_CERT_FILE'] = certifi.where()
ssl._create_default_https_context = ssl._create_unverified_context
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def get_sort_key(article):
    """ê¸°ì‚¬ ì •ë ¬ì„ ìœ„í•œ í‚¤ í•¨ìˆ˜ - ìµœì‹ ìˆœ ì •ë ¬"""
    published_date = article.get("publishedAt", "")
    if published_date:
        try:
            # ì´ë¯¸ ISO formatì´ë¯€ë¡œ ë°”ë¡œ íŒŒì‹±
            if published_date.endswith('Z'):
                published_date = published_date.replace('Z', '+00:00')
            return datetime.fromisoformat(published_date)
        except Exception as e:
            print(f"âš ï¸ Date parsing error for article: {article.get('title', '')[:30]}... - {e}")
            return datetime.min
    return datetime.min


def decode_google_news_url(source_url: str) -> str:
    """Google Newsì˜ CBMi... í˜•íƒœì˜ URLì„ ë””ì½”ë”©í•˜ì—¬ ì‹¤ì œ URLì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    try:
        if not source_url.startswith("https://news.google.com/rss/articles/"):
            return source_url

        prefix = "https://news.google.com/rss/articles/"
        base64_str = source_url[len(prefix):].split('?')[0]

        # Base64 íŒ¨ë”© ë³´ì • ë° ë””ì½”ë”©
        padding = '=' * (4 - len(base64_str) % 4)
        decoded_bytes = base64.b64decode(base64_str + padding)

        # ë””ì½”ë”©ëœ ë°”ì´íŠ¸ì—ì„œ URL íŒ¨í„´ ì¶”ì¶œ (ë³´í†µ 4ë²ˆì§¸ ë°”ì´íŠ¸ ì´í›„ì— URLì´ ìœ„ì¹˜í•¨)
        # í”„ë¡œí† ì½œ(http) ìœ„ì¹˜ë¥¼ ì°¾ì•„ ê·¸ ì§€ì ë¶€í„° ì¶”ì¶œí•˜ëŠ” ê²ƒì´ ê°€ì¥ ì•ˆì •ì ì…ë‹ˆë‹¤.
        decoded_str = decoded_bytes.decode('utf-8', errors='ignore')
        if "http" in decoded_str:
            start_idx = decoded_str.find("http")
            # URL ëë¶€ë¶„ì˜ ë¶ˆí•„ìš”í•œ ë°”ì´ë„ˆë¦¬ ë¬¸ì ì œê±°
            actual_url = ""
            for char in decoded_str[start_idx:]:
                if ord(char) < 32 or ord(char) > 126: # ì œì–´ ë¬¸ìë‚˜ ë¹„ ASCII ë¬¸ìì—ì„œ ì¤‘ë‹¨
                    break
                actual_url += char
            return actual_url
    except Exception as e:
        print(f"âš ï¸ URL Decoding failed: {e}")
    return source_url


def extract_news_content(article_url):
    """ì‹¤ì œ ë‰´ìŠ¤ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    try:
        # 1. Google News URLì¸ ê²½ìš° ì‹¤ì œ ì–¸ë¡ ì‚¬ URLë¡œ ë¨¼ì € ë³€í™˜
        real_url = decode_google_news_url(article_url)

        if real_url == article_url:
            # ë””ì½”ë”© ì‹¤íŒ¨ ì‹œ requestsë¥¼ ì´ìš©í•œ ìµœì¢… ë¦¬ë‹¤ì´ë ‰íŠ¸ ì¶”ì  ì‹œë„
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}
            with requests.Session() as s:
                s.verify = False
                resp = s.get(article_url, headers=headers, timeout=5, allow_redirects=True)
                real_url = resp.url

        print(f"ğŸ”— Attempting extraction from: {real_url}")

        # 2. newspaper3k ì„¤ì • (SSL ê²€ì¦ ìš°íšŒ ì„¤ì •ì€ ì—†ìœ¼ë¯€ë¡œ URL ìì²´ê°€ ì¤‘ìš”í•¨)
        config = Config()
        config.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        config.request_timeout = 15

        article = Article(real_url, config=config, language='ko')
        article.download()
        article.parse()

        if len(article.text) > 100:
            content = article.text.strip()
            print(f"âœ… Newspaper3k extracted content: {len(content)} characters")
            return content
        else:
            print(f"âŒ Newspaper3k failed to extract meaningful content")
            return None

    except Exception as e:
        print(f"âŒ Newspaper3k error: {e}")
        return None




# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


# ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
DATABASE_URL = os.getenv("DATABASE_URL") or "sqlite:///./news.db"

engine = create_engine(DATABASE_URL, echo=True)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

# ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë¸
class Post(Base):
    __tablename__ = "posts"

    id = Column(Integer, primary_key=True, index=True)
    title = Column(String, nullable=False)
    summary = Column(Text, nullable=False)
    content = Column(Text, nullable=False)
    category = Column(String, nullable=False)
    image_url = Column("image_url", String, nullable=False)
    created_at = Column("created_at", TIMESTAMP, server_default=func.now())

# Pydantic ìŠ¤í‚¤ë§ˆ
class PostBase(BaseModel):
    title: str
    summary: str
    content: str
    category: str
    image_url: str

class PostCreate(PostBase):
    pass

class PostResponse(PostBase):
    id: int
    created_at: Optional[datetime] = None

    @field_serializer('created_at')
    def serialize_created_at(self, value: Optional[datetime]) -> Optional[str]:
        if value is None:
            return None
        return value.isoformat()

    model_config = {
        "from_attributes": True,
        "populate_by_name": True
    }


# lifespan ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
@asynccontextmanager
async def lifespan(app: FastAPI) -> AsyncGenerator[None, None]:
    # startup
    Base.metadata.create_all(bind=engine)
    await seed_database()
    yield
    # shutdown (í•„ìš”ì‹œ cleanup ì½”ë“œ ì¶”ê°€)


# FastAPI ì•± ìƒì„±
app = FastAPI(title="News API", version="1.0.0", lifespan=lifespan)

# CORS ì„¤ì • (í•„ìš”ì‹œ)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://127.0.0.1:5173"],  # Vite ê¸°ë³¸ í¬íŠ¸
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ ì˜ì¡´ì„±
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


# ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” (í…Œì´ë¸” ìƒì„±)



async def seed_database():
    db = SessionLocal()
    try:
        # ê¸°ì¡´ ë°ì´í„° ëª¨ë‘ ì‚­ì œ (ìŠ¤í‚¤ë§ˆ ë³€ê²½ìœ¼ë¡œ ì¸í•œ ë¦¬ì…‹)
        db.query(Post).delete()
        db.commit()
        print("Existing posts deleted for schema update")
        # ì‹œë“œ ë°ì´í„°
        seed_posts = [
            {
                "title": "2025ë…„ AIì˜ ë¯¸ë˜ ì „ë§",
                "summary": "ì¸ê³µì§€ëŠ¥ì´ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë‚´ë…„ì— ì–´ë–¤ ë³€í™”ê°€ ì˜ˆìƒë˜ëŠ”ì§€ ì•Œì•„ë³´ì„¸ìš”.",
                "content": "ì¸ê³µì§€ëŠ¥ì´ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë‚´ë…„ì— ì–´ë–¤ ë³€í™”ê°€ ì˜ˆìƒë˜ëŠ”ì§€ ì•Œì•„ë³´ì„¸ìš”. ì „ë¬¸ê°€ë“¤ì€ ìƒì„±í˜• ëª¨ë¸ê³¼ ììœ¨ ì—ì´ì „íŠ¸ ë¶„ì•¼ì—ì„œ ì£¼ìš” ëŒíŒŒêµ¬ë¥¼ ì˜ˆìƒí•˜ê³  ìˆìŠµë‹ˆë‹¤. AIì˜ ì¼ìƒìƒí™œ í†µí•©ì´ ë”ìš± ì›í™œí•´ì§€ë©° ì˜ë£Œ, ê¸ˆìœµ ë“± ë‹¤ì–‘í•œ ì‚°ì—…ì— ì˜í–¥ì„ ë¯¸ì¹  ê²ƒì…ë‹ˆë‹¤.",
                "category": "ê¸°ìˆ ",
                "image_url": "https://images.unsplash.com/photo-1677442136019-21780ecad995?auto=format&fit=crop&q=80&w=800",
            },
            {
                "title": "ì¸í”Œë ˆì´ì…˜ ì™„í™”ë¡œ ê¸€ë¡œë²Œ ì¦ì‹œ ìƒìŠ¹",
                "summary": "ì´ë²ˆ ì£¼ ê²½ì œ ì§€í‘œ í˜¸ì¡°ë¡œ ì£¼ì‹ ì‹œì¥ì´ ì‹ ê³ ì ì„ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤.",
                "content": "ì´ë²ˆ ì£¼ ê²½ì œ ì§€í‘œ í˜¸ì¡°ë¡œ ì£¼ì‹ ì‹œì¥ì´ ì‹ ê³ ì ì„ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤. íˆ¬ììë“¤ì€ ì¤‘ì•™ì€í–‰ì˜ ë‹¤ìŒ ì›€ì§ì„ì— ëŒ€í•´ ë‚™ê´€ì ì…ë‹ˆë‹¤. ê¸°ìˆ ì£¼ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì£¼ìš” ì§€ìˆ˜ê°€ ì‚¬ìƒ ìµœê³ ì¹˜ë¥¼ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤.",
                "category": "ë¹„ì¦ˆë‹ˆìŠ¤",
                "image_url": "https://images.unsplash.com/photo-1611974765270-ca12586343bb?auto=format&fit=crop&q=80&w=800",
            },
            {
                "title": "ìƒëª…ì¡´ì— ìœ„ì¹˜í•œ ìƒˆë¡œìš´ í–‰ì„± ë°œê²¬",
                "summary": "ì²œë¬¸í•™ìë“¤ì´ ì§€êµ¬ì™€ ìœ ì‚¬í•œ ì ì¬ì  í–‰ì„±ì„ 40ê´‘ë…„ ê±°ë¦¬ì—ì„œ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.",
                "content": "ì²œë¬¸í•™ìë“¤ì´ ì§€êµ¬ì™€ ìœ ì‚¬í•œ ì ì¬ì  í–‰ì„±ì„ 40ê´‘ë…„ ê±°ë¦¬ì—ì„œ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ê¸€ë¦¬ì œ 12 bë¡œ ëª…ëª…ëœ ì´ í–‰ì„±ì€ ì ìƒ‰ ì™œì„± ì£¼ìœ„ë¥¼ ê³µì „í•˜ë©° ì•¡ì²´ ë¬¼ì„ ìœ ì§€í•  ìˆ˜ ìˆëŠ” ì˜¨ë„ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì œì„ìŠ¤ ì›¹ ìš°ì£¼ ë§ì›ê²½ì„ í†µí•œ ì¶”ê°€ ê´€ì¸¡ì´ ê³„íšë˜ì–´ ìˆìŠµë‹ˆë‹¤.",
                "category": "ê³¼í•™",
                "image_url": "https://images.unsplash.com/photo-1451187580459-43490279c0fa?auto=format&fit=crop&q=80&w=800",
            },
            {
                "title": "ì˜¤ëŠ˜ ë°¤ ìˆ™ë©´ì„ ìœ„í•œ 5ê°€ì§€ íŒ",
                "summary": "ìˆ™ë©´ì„ ì·¨í•˜ê¸° ì–´ë ¤ìš°ì‹ ê°€ìš”? ê³¼í•™ì ìœ¼ë¡œ ê²€ì¦ëœ íŒì„ í™•ì¸í•˜ì„¸ìš”.",
                "content": "ìˆ™ë©´ì„ ì·¨í•˜ê¸° ì–´ë ¤ìš°ì‹ ê°€ìš”? ê³¼í•™ì ìœ¼ë¡œ ê²€ì¦ëœ íŒì„ í™•ì¸í•˜ì„¸ìš”. 1. ê·œì¹™ì ì¸ ì¼ì • ìœ ì§€í•˜ê¸°. 2. í¸ì•ˆí•œ í™˜ê²½ ì¡°ì„±í•˜ê¸°. 3. ì·¨ì¹¨ ì „ í™”ë©´ ì‹œê°„ ì œí•œí•˜ê¸°. 4. ë¨¹ëŠ” ìŒì‹ê³¼ ë§ˆì‹œëŠ” ìŒë£Œ ì£¼ì˜í•˜ê¸°. 5. ì¼ìƒ ìƒí™œì— ì‹ ì²´ í™œë™ í¬í•¨í•˜ê¸°.",
                "category": "ê±´ê°•",
                "image_url": "https://images.unsplash.com/photo-1541781777621-794453259724?auto=format&fit=crop&q=80&w=800",
            },
            {
                "title": "ì˜¬ì—¬ë¦„ ë³¼ë§Œí•œ ê¸°ëŒ€ì‘ ì˜í™”ë“¤",
                "summary": "íŒì½˜ ì¤€ë¹„í•˜ì„¸ìš”! ì´ë²ˆ ì‹œì¦Œ ê°€ì¥ ê¸°ëŒ€ë˜ëŠ” ì˜í™”ë“¤ì„ ì†Œê°œí•©ë‹ˆë‹¤.",
                "content": "íŒì½˜ ì¤€ë¹„í•˜ì„¸ìš”! ì´ë²ˆ ì‹œì¦Œ ê°€ì¥ ê¸°ëŒ€ë˜ëŠ” ì˜í™”ë“¤ì„ ì†Œê°œí•©ë‹ˆë‹¤. ìŠˆí¼íˆì–´ë¡œ ëŒ€ì‘ë¶€í„° ë”°ëœ»í•œ ê°ë™ ì• ë‹ˆë©”ì´ì…˜ê¹Œì§€ ëª¨ë‘ë¥¼ ìœ„í•œ ì‘í’ˆì´ ì¤€ë¹„ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ë²ˆ ì—¬ë¦„ ê·¹ì¥ì—ì„œ ë³¼ ìˆ˜ ìˆëŠ” í•„ëŒ ì˜í™” ëª©ë¡ì„ í™•ì¸í•´ë³´ì„¸ìš”.",
                "category": "ì—”í„°í…Œì¸ë¨¼íŠ¸",
                "image_url": "https://images.unsplash.com/photo-1536440136628-849c177e76a1?auto=format&fit=crop&q=80&w=800",
            },
        ]

        for post_data in seed_posts:
            post = Post(**post_data)
            db.add(post)
        db.commit()
        print("Database seeded with initial posts")
    except Exception as e: 
        db.rollback()
        print(f"Error seeding database: {e}")
    finally:
        db.close()



# Google News RSS Client
class GoogleNewsRSSClient:
    def __init__(self):
        # í•œêµ­ ë‰´ìŠ¤ RSS í”¼ë“œ
        self.base_url = "https://news.google.com/rss"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })

            # SSL ì¸ì¦ì„œ ê²€ì¦ ë¹„í™œì„±í™” ì¶”ê°€
        self.session.verify = False

    def extract_article_content(self, url: str) -> str:
        """ë‰´ìŠ¤ ê¸°ì‚¬ URLì—ì„œ ì „ì²´ ë‚´ìš©ì„ ì¶”ì¶œ"""
        try:
            # ìš”ì²­ ê°„ ë”œë ˆì´ ì¶”ê°€ (í¬ë¡¤ë§ ì˜ˆì˜)
            time.sleep(1)

            response = self.session.get(url, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')

            # ë‹¤ì–‘í•œ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì˜ ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„
            content_selectors = [
                'article',  # ì¼ë°˜ì ì¸ article íƒœê·¸
                '[class*="content"]',  # content í´ë˜ìŠ¤ê°€ í¬í•¨ëœ ìš”ì†Œ
                '[class*="article"]',  # article í´ë˜ìŠ¤ê°€ í¬í•¨ëœ ìš”ì†Œ
                '[class*="story"]',  # story í´ë˜ìŠ¤ê°€ í¬í•¨ëœ ìš”ì†Œ
                'div[itemprop="articleBody"]',  # schema.org ë§ˆí¬ì—…
                '.news-content',  # ë„¤ì´ë²„ ë‰´ìŠ¤
                '#articleBodyContents',  # ë‹¤ìŒ ë‰´ìŠ¤
                '.article-body',  # ì¼ë°˜ì ì¸ ë³¸ë¬¸ í´ë˜ìŠ¤
                'p'  # ëª¨ë“  p íƒœê·¸ (fallback)
            ]

            for selector in content_selectors:
                content_elements = soup.select(selector)
                if content_elements:
                    # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ë¦¬
                    content_text = ' '.join([elem.get_text().strip() for elem in content_elements if elem.get_text().strip()])

                    # ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±° (ê´‘ê³ , ê´€ë ¨ ê¸°ì‚¬ ë“±)
                    content_text = re.sub(r'â–¶.*?\n', '', content_text)  # ë„¤ì´ë²„ ë‰´ìŠ¤ í™”ì‚´í‘œ ì œê±°
                    content_text = re.sub(r'\[.*?\]', '', content_text)  # ëŒ€ê´„í˜¸ ì•ˆ í…ìŠ¤íŠ¸ ì œê±°
                    content_text = re.sub(r'ì‚¬ì§„.*?\n', '', content_text)  # ì‚¬ì§„ ì„¤ëª… ì œê±°
                    content_text = re.sub(r'\s+', ' ', content_text)  # ì—°ì†ëœ ê³µë°± ì œê±°

                    if len(content_text) > 100:  # ì¶©ë¶„í•œ ê¸¸ì´ì˜ ë‚´ìš©ì¸ì§€ í™•ì¸
                        return content_text[:2000]  # ê¸¸ì´ ì œí•œ

            return ""  # ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨

        except Exception as e:
            print(f"Error extracting content from {url}: {e}")
            return ""

    def _extract_real_url(self, google_news_url: str) -> str:
        """Google News URLì—ì„œ ì‹¤ì œ ë‰´ìŠ¤ URL ì¶”ì¶œ (ê°„ì†Œí™”ëœ ë²„ì „)"""
        # ìƒˆë¡œ ë§Œë“  ì „ë¬¸ ë””ì½”ë” ì‚¬ìš©
        return decode_google_news_url(google_news_url)

    def get_news_by_topic(self, topic: str = "general") -> List[Dict]:
        """Google News ê²€ìƒ‰ RSSì—ì„œ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
        # í† í”½ë³„ ê²€ìƒ‰ì–´ ë§¤í•‘ (ë” ì•ˆì •ì ì¸ ë°©ì‹)
        topic_queries = {
            "business": "ë¹„ì¦ˆë‹ˆìŠ¤ OR ê²½ì œ OR ê¸°ì—… OR ê¸ˆìœµ",
            "technology": "ê¸°ìˆ  OR IT OR ì¸ê³µì§€ëŠ¥ OR ìŠ¤íƒ€íŠ¸ì—…",
            "science": "ê³¼í•™ OR ì—°êµ¬ OR ìš°ì£¼ OR í™˜ê²½",
            "health": "ê±´ê°• OR ì˜ë£Œ OR ë³‘ì› OR ì½”ë¡œë‚˜",
            "entertainment": "ì—°ì˜ˆ OR ì˜í™” OR ìŒì•… OR ë“œë¼ë§ˆ",
            "general": ""
        }

        # Google News ê²€ìƒ‰ RSS URL ìƒì„±
        base_url = "https://news.google.com/rss/search?q="
        query = topic_queries.get(topic, "")
        if query:
            # ê²€ìƒ‰ì–´ë¥¼ URL ì¸ì½”ë”©
            import urllib.parse
            encoded_query = urllib.parse.quote(query)
            rss_url = f"{base_url}{encoded_query}&hl=ko&gl=KR&ceid=KR:ko"
        else:
            rss_url = "https://news.google.com/rss?hl=ko&gl=KR&ceid=KR:ko"

        try:
            # RSS í”¼ë“œ íŒŒì‹±
            print(f"ğŸŒ Fetching RSS from: {rss_url}")  # ë””ë²„ê¹… ë¡œê·¸

            # SSL ê²€ì¦ ì—†ì´ RSS ê°€ì ¸ì˜¤ê¸° (requests ì‚¬ìš©)
            response = self.session.get(rss_url, verify=False)
            response.raise_for_status()
            rss_content = response.text

            # ê°€ì ¸ì˜¨ RSS í…ìŠ¤íŠ¸ë¥¼ feedparserë¡œ íŒŒì‹±
            feed = feedparser.parse(rss_content)

            # ìƒì„¸í•œ ë””ë²„ê¹… ì •ë³´
            print(f"ğŸ“¡ Feed status: {feed.status if hasattr(feed, 'status') else 'unknown'}")
            print(f"ğŸ“° Feed entries count: {len(feed.entries)}")
            print(f"ğŸ“ Feed title: {getattr(feed.feed, 'title', 'No title')}")
            print(f"ğŸ” Feed keys: {list(feed.keys())}")
            print(f"ğŸ“„ Raw feed data (first 500 chars): {str(feed)[:500]}")

            if hasattr(feed, 'bozo') and feed.bozo:
                print(f"âš ï¸ Feed parsing error: {feed.bozo_exception}")

            # entries ìƒì„¸ ì •ë³´
            if feed.entries:
                print(f"âœ… First entry keys: {list(feed.entries[0].keys()) if feed.entries else 'No entries'}")
                print(f"âœ… First entry title: {getattr(feed.entries[0], 'title', 'No title') if feed.entries else 'No entries'}")
            else:
                print(f"âŒ No entries found in feed")

            articles = []
            for entry in feed.entries[:20]:  # ìµœëŒ€ 20ê°œ ë‰´ìŠ¤
                # ì´ë¯¸ì§€ URL ì¶”ì¶œ ê°œì„ 
                image_url = ""
                if hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
                    image_url = entry.media_thumbnail[0].get('url', '')
                elif hasattr(entry, 'media_content') and entry.media_content:
                    image_url = entry.media_content[0].get('url', '')
                elif hasattr(entry, 'enclosures') and entry.enclosures:
                    for enclosure in entry.enclosures:
                        if enclosure.get('type', '').startswith('image/'):
                            image_url = enclosure.get('url', '')
                            break

                # ë‚ ì§œ ì²˜ë¦¬ ê°œì„ 
                published_at = getattr(entry, 'published', '')
                if published_at:
                    try:
                        from email.utils import parsedate_to_datetime
                        published_at = parsedate_to_datetime(published_at).isoformat()
                    except:
                        published_at = datetime.now().isoformat()

                # Google News ë§í¬ì—ì„œ ì‹¤ì œ ë‰´ìŠ¤ URL ì¶”ì¶œ ì‹œë„
                actual_url = self._extract_real_url(entry.link)

                article = {
                    "title": entry.title,
                    "description": getattr(entry, 'summary', ''),
                    "content": getattr(entry, 'summary', ''),  # RSSì—ì„œëŠ” ì½˜í…ì¸ ê°€ ì œí•œì 
                    "url": actual_url,  # ì‹¤ì œ ë‰´ìŠ¤ URL ì‚¬ìš©
                    "urlToImage": image_url,
                    "publishedAt": published_at
                }
                articles.append(article)

            print(f"âœ… Returning {len(articles)} articles")
            return articles

        except Exception as e:
            print(f"ğŸ’¥ Error parsing RSS feed for {topic}: {e}")
            import traceback
            print(f"ğŸ’¥ Full traceback: {traceback.format_exc()}")
            return []
        







# News fetch, save Func
async def fetch_and_store_news(db: Session):
    """Google News RSSì—ì„œ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì™€ì„œ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
    client = GoogleNewsRSSClient()
    
    # ì—¬ëŸ¬ ì¹´í…Œê³ ë¦¬ì˜ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
    categories = ["business", "technology", "science", "health", "entertainment"]
    
    total_processed = 0
    total_saved = 0

    for category in categories:
        print(f"ğŸ” Fetching {category} news...")  # ë””ë²„ê¹… ë¡œê·¸
        articles = client.get_news_by_topic(topic=category)
        print(f"ğŸ“Š Found {len(articles)} articles for {category}")  # ë””ë²„ê¹… ë¡œê·¸

        # ìµœì‹ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  20ê°œë¡œ ì œí•œ
        print(f"ğŸ”¢ Before sorting: {len(articles)} articles")  # ë””ë²„ê¹… ë¡œê·¸
        try:
            articles = sorted(articles, key=get_sort_key, reverse=True)[:20]
            print(f"âœ… After sorting and limiting: {len(articles)} articles")  # ë””ë²„ê¹… ë¡œê·¸
        except Exception as sort_err:
            print(f"âŒ Sorting failed: {sort_err}")  # ë””ë²„ê¹… ë¡œê·¸
            # ì •ë ¬ ì‹¤íŒ¨ì‹œ ê·¸ëƒ¥ ì²˜ìŒ 20ê°œ ì‚¬ìš©
            articles = articles[:20]
        print(f"ğŸ“Š Processing {len(articles)} most recent articles for {category}")  # ë””ë²„ê¹… ë¡œê·¸

        try:

            for i, article in enumerate(articles):
                title = article.get("title", "").strip()
                description = article.get("description", "").strip()

                # HTML íƒœê·¸ ì œê±° (Google News RSSëŠ” HTML í˜•ì‹ì˜ descriptionì„ ì œê³µ)
                if description:
                    soup = BeautifulSoup(description, 'html.parser')
                    # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ê³  ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
                    description = soup.get_text().strip()
                    # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ í†µí•©
                    description = ' '.join(description.split())

                total_processed += 1
                print(f"ğŸ“° Processing article {i+1}: {title[:50]}...")  # ë””ë²„ê¹… ë¡œê·¸
                print(f"ğŸ“ Description after cleaning: {description[:100]}...")  # ë””ë²„ê¹… ë¡œê·¸

                # ì‹¤ì œ ë‰´ìŠ¤ ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„
                news_url = ""
                full_content = None

                # Google Newsì—ì„œ ì‹¤ì œ ë‰´ìŠ¤ URL ì¶”ì¶œ (GoogleNewsRSSClientì—ì„œ "url" í•„ë“œì— ì €ì¥ë¨)
                news_url = article.get("url", "")
                print(f"ğŸ”— News URL: {news_url}")  # ë””ë²„ê¹… ë¡œê·¸

                # ì‹¤ì œ ë‰´ìŠ¤ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ
                if news_url:
                    full_content = extract_news_content(news_url)

                # content ì„¤ì • (ì‹¤ì œ ë³¸ë¬¸ ìš°ì„ , ì—†ìœ¼ë©´ description ì‚¬ìš©)
                if full_content:
                    content = full_content
                    print(f"ğŸ“„ Using full article content ({len(content)} chars)")  # ë””ë²„ê¹… ë¡œê·¸
                else:
                    content = description
                    print(f"ğŸ“„ Using RSS description ({len(content)} chars)")  # ë””ë²„ê¹… ë¡œê·¸

                # ìœ íš¨ì„± ê²€ì¦
                if not title or not content:
                    print(f"âŒ Skipped: Empty title or content")  # ë””ë²„ê¹… ë¡œê·¸
                    continue
                
                # ì¤‘ë³µ ì²´í¬
                published_date = article.get("publishedAt", "")
                existing = None
                
                if published_date:
                    try:
                        from datetime import datetime
                        pub_dt = datetime.fromisoformat(published_date.replace('Z', '+00:00'))
                        date_str = pub_dt.date().isoformat()
                        existing = db.query(Post).filter(
                            Post.title == title,
                            func.date(Post.created_at) == date_str
                        ).first()
                    except Exception as date_err:
                        print(f"âš ï¸ Date parsing error: {date_err}")  # ë””ë²„ê¹… ë¡œê·¸
                        existing = db.query(Post).filter(Post.title == title).first()
                else:
                    existing = db.query(Post).filter(Post.title == title).first()
                
                if existing:
                    print(f"ğŸ”„ Skipped: Already exists - {title[:30]}...")  # ë””ë²„ê¹… ë¡œê·¸
                    continue
                
                # ì €ì¥í•  ë°ì´í„° ì¤€ë¹„
                # news_urlì€ ìœ„ì—ì„œ ì´ë¯¸ ì¶”ì¶œë¨

                print(f"ğŸ“ Original content length: {len(content)}")  # ë””ë²„ê¹… ë¡œê·¸
                print(f"ğŸ”— News URL: {news_url}")  # ë””ë²„ê¹… ë¡œê·¸

                # RSS ë‚´ìš©ì´ ë¶€ì¡±í•˜ê±°ë‚˜ ë§í¬ë§Œ ìˆìœ¼ë©´ ì‹¤ì œ ê¸°ì‚¬ì—ì„œ ì „ì²´ ë‚´ìš© ì¶”ì¶œ ì‹œë„
                should_extract = (
                    len(content) < 200 or      # ë‚´ìš©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜
                    "http" in content or        # ë§í¬ê°€ í¬í•¨ë˜ì–´ ìˆê±°ë‚˜
                    "..." in content or         # ìƒëµ ê¸°í˜¸ê°€ ìˆê±°ë‚˜
                    content.strip() == "" or    # ë‚´ìš©ì´ ë¹„ì–´ìˆê±°ë‚˜
                    len(content.split()) < 10   # ë‹¨ì–´ê°€ 10ê°œ ë¯¸ë§Œ
                )

                if should_extract and news_url:
                    print(f"ğŸ› ï¸ Extracting full content from: {news_url}")  # ë””ë²„ê¹… ë¡œê·¸
                    try:
                        full_article_content = client.extract_article_content(news_url)
                        if full_article_content and len(full_article_content) > len(content):
                            content = full_article_content
                            print(f"âœ… Successfully extracted content ({len(content)} chars)")  # ë””ë²„ê¹… ë¡œê·¸
                        else:
                            print(f"âŒ Failed to extract content or content too short")  # ë””ë²„ê¹… ë¡œê·¸
                            # ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ì›ë³¸ contentë¼ë„ ì‚¬ìš© (ë§í¬ ì œê±°)
                            if "http" in content:
                                content = content.split("http")[0].strip()
                                print(f"ğŸ”§ Cleaned content: {content[:100]}...")  # ë””ë²„ê¹… ë¡œê·¸
                    except Exception as extract_err:
                        print(f"âš ï¸ Content extraction failed: {extract_err}")  # ë””ë²„ê¹… ë¡œê·¸
                        # ì—ëŸ¬ ì‹œì—ë„ ë§í¬ ì œê±°ëœ content ì‚¬ìš©
                        if "http" in content:
                            content = content.split("http")[0].strip()
                
                if not content:
                    print(f"âŒ Skipped: No content available")  # ë””ë²„ê¹… ë¡œê·¸
                    continue
                
                full_content = content
                if news_url:
                    full_content += f"\n\nğŸ”— ì „ì²´ ê¸°ì‚¬ ë³´ê¸°: {news_url}"
                
                image_url = article.get("urlToImage", "")
                if not image_url:
                    image_url = "https://images.unsplash.com/photo-1504711434969-e33886168f5c?auto=format&fit=crop&q=80&w=800"
                
                post_data = {
                    "title": title[:200],
                    "summary": description[:300],
                    "content": full_content,
                    "category": category.capitalize(),
                    "image_url": image_url
                }
                
                try:
                    db_post = Post(**post_data)
                    db.add(db_post)
                    total_saved += 1
                    print(f"âœ… Saved article: {title[:30]}...")  # ë””ë²„ê¹… ë¡œê·¸
                except Exception as save_err:
                    print(f"âŒ Save failed: {save_err}")  # ë””ë²„ê¹… ë¡œê·¸
                    continue
                
        except Exception as e:
            print(f"ğŸ’¥ Error fetching {category} news: {e}")
            continue

    try:
        db.commit()
        print(f"ğŸ‰ Total processed: {total_processed}, Total saved: {total_saved}")  # ìµœì¢… ê²°ê³¼ ë¡œê·¸
        print("News fetched and stored successfully")
    except Exception as e:
        db.rollback()
        print(f"ğŸ’¥ Error saving news to database: {e}")

# API ì•¤ë“œ í¬ì¸íŠ¸ë“¤
@app.get("/api/posts", response_model=List[PostResponse])
async def get_posts(
    category: Optional[str] = Query(None),
    search: Optional[str] = Query(None),
    db: Session = Depends(get_db)
):
    query = db.query(Post)

    if category:
        query = query.filter(Post.category == category)

    if search:
        search_term = f"%{search.lower()}%"
        query = query.filter(
            (Post.title.ilike(search_term)) |
            (Post.content.ilike(search_term))
        )

    posts = query.order_by(Post.created_at.desc()).all()
    return posts

# FastAPIì—ì„œëŠ” ê²½ë¡œ íŒŒë¼ë¯¸í„°ë¥¼ ì¤‘ê´„í˜¸ë¡œ ì„ ì–¸í•´ì•¼ í•˜ë©°, f-stringì„ ì‚¬ìš©í•  í•„ìš”ê°€ ì—†ë‹¤.
@app.api_route("/api/posts/{post_id}", methods=["GET"])  # api_routeë¡œ ë³€ê²½í•˜ì—¬ validation ìš°íšŒ
async def get_post(post_id, db: Session = Depends(get_db)):  # íƒ€ì… íŒíŠ¸ ì œê±°
    print(f"DEBUG: Requesting post with ID: {post_id}, type: {type(post_id)}")

    try:
        post_id_int = int(post_id)
        print(f"DEBUG: Converted to int: {post_id_int}")
    except ValueError as e:
        print(f"DEBUG: Failed to convert ID to int: {e}")
        raise HTTPException(status_code=400, detail=f"Invalid ID format: {post_id}")

    # ë°ì´í„°ë² ì´ìŠ¤ì— í•´ë‹¹ IDê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    all_posts = db.query(Post).all()
    print(f"DEBUG: All post IDs in database: {[p.id for p in all_posts]}")

    post = db.query(Post).filter(Post.id == post_id_int).first()
    if not post:
        print(f"DEBUG: Post with ID {post_id_int} not found")
        raise HTTPException(status_code=404, detail="Post not found")

    print(f"DEBUG: Found post: {post.id}, {post.title}")
    return post


@app.post("/api/posts", response_model=PostResponse, status_code=201)
async def create_post(post: PostCreate, db: Session = Depends(get_db)):
    db_post = Post(**post.dict())
    db.add(db_post)
    db.commit()
    db.refresh(db_post)
    return db_post


@app.post("/api/news/fetch")
async def fetch_latest_news(db: Session = Depends(get_db)):
    """ìµœì‹  ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì™€ì„œ ì €ì¥"""
    await fetch_and_store_news(db)
    return {"message": "Latest news fetched and stored successfully"}
    
        

if __name__ == "__main__":
    port = int(os.getenv("PORT", 8000))  # 5000 ëŒ€ì‹  8000 ì‚¬ìš©
    uvicorn.run(app, host="127.0.0.1", port=port)



