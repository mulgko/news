from fastapi import FastAPI, HTTPException, Query, Depends
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field, field_serializer
from datetime import datetime
from typing import Optional, List, AsyncGenerator, Dict
from contextlib import asynccontextmanager
import os
from dotenv import load_dotenv
from sqlalchemy import create_engine, Column, Integer, String, Text, TIMESTAMP, func
from sqlalchemy.orm import declarative_base
from sqlalchemy.orm import sessionmaker, Session
import uvicorn
import feedparser
import requests
from bs4 import BeautifulSoup
import time
import re
import trafilatura

# ê°„ë‹¨ ë²„ì „ì—ì„œëŠ” ê¸°ë³¸ ì„¸ì…˜ë§Œ ì‚¬ìš©
session = requests.Session()

def get_sort_key(article):
    """ê¸°ì‚¬ ì •ë ¬ì„ ìœ„í•œ í‚¤ í•¨ìˆ˜ - ìµœì‹ ìˆœ ì •ë ¬"""
    published_date = article.get("publishedAt", "")
    if published_date:
        try:
            # ì´ë¯¸ ISO formatì´ë¯€ë¡œ ë°”ë¡œ íŒŒì‹±
            if published_date.endswith('Z'):
                published_date = published_date.replace('Z', '+00:00')
            return datetime.fromisoformat(published_date)
        except Exception as e:
            print(f"âš ï¸ Date parsing error for article: {article.get('title', '')[:30]}... - {e}")
            return datetime.min
    return datetime.min


def decode_google_news_url(url: str, session=None) -> str:
    """
    Google News URL ë””ì½”ë”© (googlenewsdecoder ìš°ì„  + ê¸°ì¡´ ë°©ë²•ë“¤)
    """
    if not url or "google.com" not in url:
        return url

    try:
        # 0. googlenewsdecoder ìš°ì„  ì‹œë„ (ê°€ì¥ íš¨ê³¼ì !)
        try:
            from googlenewsdecoder import new_decoderv1
            decoded = new_decoderv1(url)
            if decoded and decoded != url and "google.com" not in decoded:
                print(f"âœ… googlenewsdecoder ì„±ê³µ: {decoded[:80]}...")
                return decoded
        except ImportError:
            print("âš ï¸ googlenewsdecoder ë¯¸ì„¤ì¹˜")
        except Exception as decoder_error:
            print(f"âš ï¸ googlenewsdecoder ì‹¤íŒ¨: {decoder_error}")

        # 1. HTTP ë¦¬ë‹¤ì´ë ‰íŠ¸ ì‹œë„
        if session is None:
            session = requests.Session()
            session.verify = False

        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
                'Referer': 'https://news.google.com/',
            }

            print(f"ğŸ”— HTTP ë¦¬ë‹¤ì´ë ‰íŠ¸ ì‹œë„...")
            response = session.get(url, headers=headers, allow_redirects=True, timeout=15, verify=False)

            final_url = response.url
            if final_url != url and "google.com" not in final_url and final_url.startswith('http'):
                print(f"âœ… HTTP ë¦¬ë‹¤ì´ë ‰íŠ¸ ì„±ê³µ: {final_url[:80]}...")
                return final_url
            else:
                print(f"âš ï¸ ë¦¬ë‹¤ì´ë ‰íŠ¸ ê²°ê³¼ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŒ: {final_url[:60]}...")

        except Exception as redirect_error:
            print(f"âš ï¸ HTTP ë¦¬ë‹¤ì´ë ‰íŠ¸ ì‹¤íŒ¨: {redirect_error}")

        # 2. Base64 ë””ì½”ë”© ì‹œë„ (ë³´ì¡° ìˆ˜ë‹¨)
        import base64
        import re

        match = re.search(r'/rss/articles/(CBMi[^?]+)', url)
        if match:
            encoded_part = match.group(1)
            print(f"ğŸ” Base64 ë””ì½”ë”© ì‹œë„...")

            try:
                # íŒ¨ë”© ì¶”ê°€
                missing_padding = len(encoded_part) % 4
                if missing_padding:
                    encoded_part += '=' * (4 - missing_padding)

                decoded_bytes = base64.urlsafe_b64decode(encoded_part)
                decoded_text = decoded_bytes.decode('utf-8', errors='ignore')

                # URL íŒ¨í„´ ì°¾ê¸°
                url_patterns = [
                    r'https?://[^\s\'"<>(){}\[\]]+',
                    r'https?://[^\s\'"<>\s]+',
                ]

                for pattern in url_patterns:
                    matches = re.findall(pattern, decoded_text)
                    for match in matches:
                        real_url = re.sub(r'[<>,"\'\s]+$', '', match)
                        if len(real_url) > 20 and "google.com" not in real_url and real_url.startswith('http'):
                            print(f"âœ… Base64ì—ì„œ URL ë°œê²¬: {real_url[:80]}...")
                            return real_url

            except Exception as b64_error:
                print(f"âš ï¸ Base64 ë””ì½”ë”© ì‹¤íŒ¨: {b64_error}")

        # 3. ìµœí›„ì˜ ìˆ˜ë‹¨: URL íŒŒë¼ë¯¸í„° ë°©ì‹
        from urllib.parse import urlparse, parse_qs
        parsed = urlparse(url)
        if 'url' in parse_qs(parsed.query):
            direct_url = parse_qs(parsed.query)['url'][0]
            if "google.com" not in direct_url and direct_url.startswith('http'):
                print(f"âœ… URL íŒŒë¼ë¯¸í„°ì—ì„œ ì¶”ì¶œ: {direct_url[:80]}...")
                return direct_url

        print(f"âš ï¸ ëª¨ë“  ë””ì½”ë”© ë°©ë²• ì‹¤íŒ¨, ì›ë³¸ URL ì‚¬ìš©")
        return url

    except Exception as e:
        print(f"ğŸ’¥ URL ë””ì½”ë”© ì˜¤ë¥˜: {e}, ì›ë³¸ ì‚¬ìš©")
        return url

def extract_news_content(article_url: str, session=None) -> str:
    """
    ê°œì„ ëœ ë‰´ìŠ¤ ë³¸ë¬¸ ì¶”ì¶œ (BeautifulSoup ìš°ì„ )
    Google News URL ë””ì½”ë”© í›„ ë³¸ë¬¸ ìë™ ì¶”ì¶œ
    """
    try:
        # 1. Google News URL ë””ì½”ë”©
        real_url = decode_google_news_url(article_url, session)

        if not real_url:
            print(f"URL ì²˜ë¦¬ ì‹¤íŒ¨: {article_url}")
            return None

        # Google News URLì¸ ê²½ìš°ì—ë„ ì‹œë„ (ë¦¬ë‹¤ì´ë ‰íŠ¸ë  ê²ƒì„)
        target_url = real_url if real_url != article_url else article_url

        # 2. BeautifulSoupë¡œ ìš°ì„  ì¶”ì¶œ ì‹œë„ (ë” ì•ˆì •ì )
        print(f"BeautifulSoupë¡œ ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„: {target_url[:80]}...")
        result = _extract_with_beautifulsoup(target_url, session)
        if result:
            return result

        # 3. BeautifulSoup ì‹¤íŒ¨ì‹œ Trafilatura ëŒ€ì•ˆ ì‹œë„
        print(f"BeautifulSoup ì‹¤íŒ¨, Trafilatura ëŒ€ì•ˆ ì‹œë„")
        downloaded = trafilatura.fetch_url(target_url)

        if not downloaded:
            print(f"í˜ì´ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {target_url}")
            return None

        # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì •ë°€ ëª¨ë“œ, ëŒ“ê¸€ ì œì™¸)
        text = trafilatura.extract(
            downloaded,
            output_format='txt',
            include_comments=False,
            favor_precision=True
        )

        if text and len(text.strip()) > 100:
            # ì„±ê³µ: í…ìŠ¤íŠ¸ ì •ë¦¬
            cleaned_text = ' '.join(text.split())  # ì—°ì† ê³µë°± ì œê±°
            print(f"Trafilatura ì¶”ì¶œ ì„±ê³µ: {len(cleaned_text)}ì")
            return cleaned_text[:2000]  # ê¸¸ì´ ì œí•œ
        else:
            print(f"Trafilatura ì¶”ì¶œ ì‹¤íŒ¨")
            return None

    except Exception as e:
        print(f"ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        # ìµœì¢… Fallback: BeautifulSoup ì¬ì‹œë„
        try:
            return _extract_with_beautifulsoup(target_url, session)
        except Exception as fallback_e:
            print(f"Fallbackë„ ì‹¤íŒ¨: {fallback_e}")
            return None


def _extract_with_beautifulsoup(url: str, session=None) -> str:
    """
    BeautifulSoupë¥¼ ì‚¬ìš©í•œ ëŒ€ì•ˆ ë³¸ë¬¸ ì¶”ì¶œ
    Trafilatura ì‹¤íŒ¨ì‹œ ì‚¬ìš©
    """
    try:
        if session is None:
            session = requests.Session()
            session.verify = False

        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }

        # SSL ê²€ì¦ ì™„ì „ ìš°íšŒ ë° íƒ€ì„ì•„ì›ƒ ì¦ê°€ - ê°•í™”ëœ SSL ì²˜ë¦¬
        try:
            response = session.get(url, headers=headers, timeout=20, verify=False, allow_redirects=True)
            response.raise_for_status()
        except Exception as ssl_error:
            print(f"âš ï¸ SSL ì˜¤ë¥˜ ë°œìƒ, ì¸ì¦ì„œ ê²€ì¦ ì™„ì „ ìš°íšŒ ì‹œë„: {ssl_error}")
            try:
                import ssl
                response = session.get(
                    url,
                    headers=headers,
                    timeout=20,
                    verify=False,
                    allow_redirects=True,
                    cert_reqs=ssl.CERT_NONE
                )
                response.raise_for_status()
            except Exception as fallback_error:
                print(f"ğŸ’¥ SSL ìš°íšŒ ì‹¤íŒ¨: {fallback_error}")
                raise fallback_error

        soup = BeautifulSoup(response.content, 'html.parser')

        # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
        for element in soup.find_all(['script', 'style', 'nav', 'footer', 'header', 'aside']):
            element.decompose()

        # í•œêµ­ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ìš© ë³¸ë¬¸ ì„ íƒìë“¤
        content_selectors = [
            'article',
            '[id*="article"]',
            '[class*="article"]',
            '[id*="content"]',
            '[class*="content"]',
            '#articleBody',
            '#newsct_article',
            '.article_body',
            '.news_body',
            'div[itemprop="articleBody"]',
            '.article-content',
            'main'
        ]

        content_text = ""
        for selector in content_selectors:
            elements = soup.select(selector)
            if elements:
                texts = []
                for elem in elements:
                    paragraphs = elem.find_all(['p', 'div'])
                    for p in paragraphs:
                        text = p.get_text(strip=True)
                        if len(text) > 30:  # ì˜ë¯¸ìˆëŠ” ê¸¸ì´ì˜ í…ìŠ¤íŠ¸ë§Œ
                            texts.append(text)

                if texts:
                    content_text = '\n\n'.join(texts)
                    break

        # ì¶”ê°€ ì •ë¦¬
        if content_text:
            # í•œêµ­ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ í”í•œ ì•„í‹°íŒ©íŠ¸ ì œê±°
            content_text = re.sub(r'â–¶.*?\n', '', content_text)
            content_text = re.sub(r'\[.*?\]', '', content_text)
            content_text = re.sub(r'ì‚¬ì§„.*?\n', '', content_text)
            content_text = re.sub(r'\s+', ' ', content_text)
            content_text = content_text.strip()

        if len(content_text) > 100:
            print(f"âœ… BeautifulSoup ì¶”ì¶œ ì„±ê³µ: {len(content_text)}ì")
            return content_text[:2000]
        else:
            print(f"âŒ BeautifulSoup ì¶”ì¶œ ì‹¤íŒ¨: í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ")
            return None

    except Exception as e:
        print(f"ğŸ’¥ BeautifulSoup ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return None




# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


# ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
DATABASE_URL = os.getenv("DATABASE_URL") or "sqlite:///./news.db"

engine = create_engine(DATABASE_URL, echo=True)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

# ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë¸
class Post(Base):
    __tablename__ = "posts"

    id = Column(Integer, primary_key=True, index=True)
    title = Column(String, nullable=False)
    summary = Column(Text, nullable=False)
    content = Column(Text, nullable=False)
    category = Column(String, nullable=False)
    image_url = Column("image_url", String, nullable=False)
    created_at = Column("created_at", TIMESTAMP, server_default=func.now())

# Pydantic ìŠ¤í‚¤ë§ˆ
class PostBase(BaseModel):
    title: str
    summary: str
    content: str
    category: str
    image_url: str

class PostCreate(PostBase):
    pass

class PostResponse(PostBase):
    id: int
    created_at: Optional[datetime] = None

    @field_serializer('created_at')
    def serialize_created_at(self, value: Optional[datetime]) -> Optional[str]:
        if value is None:
            return None
        return value.isoformat()

    model_config = {
        "from_attributes": True,
        "populate_by_name": True
    }


# lifespan ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
@asynccontextmanager
async def lifespan(app: FastAPI) -> AsyncGenerator[None, None]:
    # startup
    Base.metadata.create_all(bind=engine)
    await seed_database()
    yield
    # shutdown (í•„ìš”ì‹œ cleanup ì½”ë“œ ì¶”ê°€)


# FastAPI ì•± ìƒì„±
app = FastAPI(title="News API", version="1.0.0", lifespan=lifespan)

# CORS ì„¤ì • (í•„ìš”ì‹œ)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://127.0.0.1:5173"],  # Vite ê¸°ë³¸ í¬íŠ¸
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ ì˜ì¡´ì„±
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


# ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” (í…Œì´ë¸” ìƒì„±)



async def seed_database():
    db = SessionLocal()
    try:
        # ê¸°ì¡´ ë°ì´í„° ëª¨ë‘ ì‚­ì œ (ìŠ¤í‚¤ë§ˆ ë³€ê²½ìœ¼ë¡œ ì¸í•œ ë¦¬ì…‹)
        db.query(Post).delete()
        db.commit()
        print("Existing posts deleted for schema update")
        # ì‹œë“œ ë°ì´í„°
        seed_posts = [
            {
                "title": "2025ë…„ AIì˜ ë¯¸ë˜ ì „ë§",
                "summary": "ì¸ê³µì§€ëŠ¥ì´ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë‚´ë…„ì— ì–´ë–¤ ë³€í™”ê°€ ì˜ˆìƒë˜ëŠ”ì§€ ì•Œì•„ë³´ì„¸ìš”.",
                "content": "ì¸ê³µì§€ëŠ¥ì´ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë‚´ë…„ì— ì–´ë–¤ ë³€í™”ê°€ ì˜ˆìƒë˜ëŠ”ì§€ ì•Œì•„ë³´ì„¸ìš”. ì „ë¬¸ê°€ë“¤ì€ ìƒì„±í˜• ëª¨ë¸ê³¼ ììœ¨ ì—ì´ì „íŠ¸ ë¶„ì•¼ì—ì„œ ì£¼ìš” ëŒíŒŒêµ¬ë¥¼ ì˜ˆìƒí•˜ê³  ìˆìŠµë‹ˆë‹¤. AIì˜ ì¼ìƒìƒí™œ í†µí•©ì´ ë”ìš± ì›í™œí•´ì§€ë©° ì˜ë£Œ, ê¸ˆìœµ ë“± ë‹¤ì–‘í•œ ì‚°ì—…ì— ì˜í–¥ì„ ë¯¸ì¹  ê²ƒì…ë‹ˆë‹¤.",
                "category": "ê¸°ìˆ ",
                "image_url": "https://images.unsplash.com/photo-1677442136019-21780ecad995?auto=format&fit=crop&q=80&w=800",
            },
            {
                "title": "ì¸í”Œë ˆì´ì…˜ ì™„í™”ë¡œ ê¸€ë¡œë²Œ ì¦ì‹œ ìƒìŠ¹",
                "summary": "ì´ë²ˆ ì£¼ ê²½ì œ ì§€í‘œ í˜¸ì¡°ë¡œ ì£¼ì‹ ì‹œì¥ì´ ì‹ ê³ ì ì„ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤.",
                "content": "ì´ë²ˆ ì£¼ ê²½ì œ ì§€í‘œ í˜¸ì¡°ë¡œ ì£¼ì‹ ì‹œì¥ì´ ì‹ ê³ ì ì„ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤. íˆ¬ììë“¤ì€ ì¤‘ì•™ì€í–‰ì˜ ë‹¤ìŒ ì›€ì§ì„ì— ëŒ€í•´ ë‚™ê´€ì ì…ë‹ˆë‹¤. ê¸°ìˆ ì£¼ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì£¼ìš” ì§€ìˆ˜ê°€ ì‚¬ìƒ ìµœê³ ì¹˜ë¥¼ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤.",
                "category": "ë¹„ì¦ˆë‹ˆìŠ¤",
                "image_url": "https://images.unsplash.com/photo-1611974765270-ca12586343bb?auto=format&fit=crop&q=80&w=800",
            },
            {
                "title": "ìƒëª…ì¡´ì— ìœ„ì¹˜í•œ ìƒˆë¡œìš´ í–‰ì„± ë°œê²¬",
                "summary": "ì²œë¬¸í•™ìë“¤ì´ ì§€êµ¬ì™€ ìœ ì‚¬í•œ ì ì¬ì  í–‰ì„±ì„ 40ê´‘ë…„ ê±°ë¦¬ì—ì„œ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.",
                "content": "ì²œë¬¸í•™ìë“¤ì´ ì§€êµ¬ì™€ ìœ ì‚¬í•œ ì ì¬ì  í–‰ì„±ì„ 40ê´‘ë…„ ê±°ë¦¬ì—ì„œ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ê¸€ë¦¬ì œ 12 bë¡œ ëª…ëª…ëœ ì´ í–‰ì„±ì€ ì ìƒ‰ ì™œì„± ì£¼ìœ„ë¥¼ ê³µì „í•˜ë©° ì•¡ì²´ ë¬¼ì„ ìœ ì§€í•  ìˆ˜ ìˆëŠ” ì˜¨ë„ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì œì„ìŠ¤ ì›¹ ìš°ì£¼ ë§ì›ê²½ì„ í†µí•œ ì¶”ê°€ ê´€ì¸¡ì´ ê³„íšë˜ì–´ ìˆìŠµë‹ˆë‹¤.",
                "category": "ê³¼í•™",
                "image_url": "https://images.unsplash.com/photo-1451187580459-43490279c0fa?auto=format&fit=crop&q=80&w=800",
            },
            {
                "title": "ì˜¤ëŠ˜ ë°¤ ìˆ™ë©´ì„ ìœ„í•œ 5ê°€ì§€ íŒ",
                "summary": "ìˆ™ë©´ì„ ì·¨í•˜ê¸° ì–´ë ¤ìš°ì‹ ê°€ìš”? ê³¼í•™ì ìœ¼ë¡œ ê²€ì¦ëœ íŒì„ í™•ì¸í•˜ì„¸ìš”.",
                "content": "ìˆ™ë©´ì„ ì·¨í•˜ê¸° ì–´ë ¤ìš°ì‹ ê°€ìš”? ê³¼í•™ì ìœ¼ë¡œ ê²€ì¦ëœ íŒì„ í™•ì¸í•˜ì„¸ìš”. 1. ê·œì¹™ì ì¸ ì¼ì • ìœ ì§€í•˜ê¸°. 2. í¸ì•ˆí•œ í™˜ê²½ ì¡°ì„±í•˜ê¸°. 3. ì·¨ì¹¨ ì „ í™”ë©´ ì‹œê°„ ì œí•œí•˜ê¸°. 4. ë¨¹ëŠ” ìŒì‹ê³¼ ë§ˆì‹œëŠ” ìŒë£Œ ì£¼ì˜í•˜ê¸°. 5. ì¼ìƒ ìƒí™œì— ì‹ ì²´ í™œë™ í¬í•¨í•˜ê¸°.",
                "category": "ê±´ê°•",
                "image_url": "https://images.unsplash.com/photo-1541781777621-794453259724?auto=format&fit=crop&q=80&w=800",
            },
            {
                "title": "ì˜¬ì—¬ë¦„ ë³¼ë§Œí•œ ê¸°ëŒ€ì‘ ì˜í™”ë“¤",
                "summary": "íŒì½˜ ì¤€ë¹„í•˜ì„¸ìš”! ì´ë²ˆ ì‹œì¦Œ ê°€ì¥ ê¸°ëŒ€ë˜ëŠ” ì˜í™”ë“¤ì„ ì†Œê°œí•©ë‹ˆë‹¤.",
                "content": "íŒì½˜ ì¤€ë¹„í•˜ì„¸ìš”! ì´ë²ˆ ì‹œì¦Œ ê°€ì¥ ê¸°ëŒ€ë˜ëŠ” ì˜í™”ë“¤ì„ ì†Œê°œí•©ë‹ˆë‹¤. ìŠˆí¼íˆì–´ë¡œ ëŒ€ì‘ë¶€í„° ë”°ëœ»í•œ ê°ë™ ì• ë‹ˆë©”ì´ì…˜ê¹Œì§€ ëª¨ë‘ë¥¼ ìœ„í•œ ì‘í’ˆì´ ì¤€ë¹„ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ë²ˆ ì—¬ë¦„ ê·¹ì¥ì—ì„œ ë³¼ ìˆ˜ ìˆëŠ” í•„ëŒ ì˜í™” ëª©ë¡ì„ í™•ì¸í•´ë³´ì„¸ìš”.",
                "category": "ì—”í„°í…Œì¸ë¨¼íŠ¸",
                "image_url": "https://images.unsplash.com/photo-1536440136628-849c177e76a1?auto=format&fit=crop&q=80&w=800",
            },
        ]

        for post_data in seed_posts:
            post = Post(**post_data)
            db.add(post)
        db.commit()
        print("Database seeded with initial posts")
    except Exception as e: 
        db.rollback()
        print(f"Error seeding database: {e}")
    finally:
        db.close()



# Google News RSS Client
class GoogleNewsRSSClient:
    def __init__(self):
        # í•œêµ­ ë‰´ìŠ¤ RSS í”¼ë“œ
        self.base_url = "https://news.google.com/rss"
        # ê°„ë‹¨ ë²„ì „ì—ì„œëŠ” ê¸°ë³¸ ì„¸ì…˜ ì‚¬ìš©
        self.session = session

    def extract_article_content(self, url: str) -> str:
        """Trafilaturaë¥¼ ì‚¬ìš©í•œ ë‰´ìŠ¤ ë³¸ë¬¸ ì¶”ì¶œ"""
        return extract_news_content(url, self.session)

    def _extract_real_url(self, google_news_url: str) -> str:
        """Google News URLì—ì„œ ì‹¤ì œ ë‰´ìŠ¤ URL ì¶”ì¶œ (ê°„ì†Œí™”ëœ ë²„ì „)"""
        # ìƒˆë¡œ ë§Œë“  ì „ë¬¸ ë””ì½”ë” ì‚¬ìš© - self.session ì „ë‹¬!
        return decode_google_news_url(google_news_url, self.session)

    def get_news_by_topic(self, topic: str = "general") -> List[Dict]:
        """Google News ê²€ìƒ‰ RSSì—ì„œ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
        # í† í”½ë³„ ê²€ìƒ‰ì–´ ë§¤í•‘ (ë” ì•ˆì •ì ì¸ ë°©ì‹)
        topic_queries = {
            "business": "ë¹„ì¦ˆë‹ˆìŠ¤ OR ê²½ì œ OR ê¸°ì—… OR ê¸ˆìœµ",
            "technology": "ê¸°ìˆ  OR IT OR ì¸ê³µì§€ëŠ¥ OR ìŠ¤íƒ€íŠ¸ì—…",
            "science": "ê³¼í•™ OR ì—°êµ¬ OR ìš°ì£¼ OR í™˜ê²½",
            "health": "ê±´ê°• OR ì˜ë£Œ OR ë³‘ì› OR ì½”ë¡œë‚˜",
            "entertainment": "ì—°ì˜ˆ OR ì˜í™” OR ìŒì•… OR ë“œë¼ë§ˆ",
            "general": ""
        }

        # Google News ê²€ìƒ‰ RSS URL ìƒì„±
        base_url = "https://news.google.com/rss/search?q="
        query = topic_queries.get(topic, "")
        if query:
            # ê²€ìƒ‰ì–´ë¥¼ URL ì¸ì½”ë”©
            import urllib.parse
            encoded_query = urllib.parse.quote(query)
            rss_url = f"{base_url}{encoded_query}&hl=ko&gl=KR&ceid=KR:ko"
        else:
            rss_url = "https://news.google.com/rss?hl=ko&gl=KR&ceid=KR:ko"

        try:
            # RSS í”¼ë“œ íŒŒì‹±
            print(f"ğŸŒ Fetching RSS from: {rss_url}")  # ë””ë²„ê¹… ë¡œê·¸

            # SSL ê²€ì¦ ì—†ì´ RSS ê°€ì ¸ì˜¤ê¸° (requests ì‚¬ìš©) - ê°•í™”ëœ SSL ìš°íšŒ
            try:
                # ì²« ë²ˆì§¸ ì‹œë„: ì¼ë°˜ì ì¸ SSL ìš°íšŒ
                response = self.session.get(rss_url, verify=False, timeout=30)
                response.raise_for_status()
                rss_content = response.text
            except Exception as ssl_error:
                print(f"âš ï¸ SSL ì˜¤ë¥˜ ë°œìƒ, ì¸ì¦ì„œ ê²€ì¦ ì™„ì „ ìš°íšŒ ì‹œë„: {ssl_error}")
                try:
                    # ë‘ ë²ˆì§¸ ì‹œë„: ë” ê°•ë ¥í•œ SSL ìš°íšŒ
                    import ssl
                    from urllib3.util import ssl_

                    # SSL ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ì¸ì¦ì„œ ê²€ì¦ ì™„ì „ ë¹„í™œì„±í™”)
                    ssl_context = ssl.create_default_context()
                    ssl_context.check_hostname = False
                    ssl_context.verify_mode = ssl.CERT_NONE

                    response = self.session.get(
                        rss_url,
                        verify=False,
                        timeout=30,
                        cert_reqs=ssl.CERT_NONE
                    )
                    response.raise_for_status()
                    rss_content = response.text
                except Exception as fallback_error:
                    print(f"ğŸ’¥ SSL ìš°íšŒ ì‹¤íŒ¨, ë§ˆì§€ë§‰ ì‹œë„: {fallback_error}")
                    # ì„¸ ë²ˆì§¸ ì‹œë„: urllib ì‚¬ìš©
                    try:
                        import urllib.request
                        import urllib.error

                        req = urllib.request.Request(rss_url)
                        req.add_header('User-Agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
                        with urllib.request.urlopen(req, timeout=30) as response:
                            rss_content = response.read().decode('utf-8')
                    except Exception as urllib_error:
                        print(f"ğŸ’¥ ëª¨ë“  SSL ìš°íšŒ ë°©ë²• ì‹¤íŒ¨: {urllib_error}")
                        return []

            # ê°€ì ¸ì˜¨ RSS í…ìŠ¤íŠ¸ë¥¼ feedparserë¡œ íŒŒì‹±
            feed = feedparser.parse(rss_content)

            # ìƒì„¸í•œ ë””ë²„ê¹… ì •ë³´
            print(f"ğŸ“¡ Feed status: {feed.status if hasattr(feed, 'status') else 'unknown'}")
            print(f"ğŸ“° Feed entries count: {len(feed.entries)}")
            print(f"ğŸ“ Feed title: {getattr(feed.feed, 'title', 'No title')}")
            print(f"ğŸ” Feed keys: {list(feed.keys())}")
            print(f"ğŸ“„ Raw feed data (first 500 chars): {str(feed)[:500]}")

            if hasattr(feed, 'bozo') and feed.bozo:
                print(f"âš ï¸ Feed parsing error: {feed.bozo_exception}")

            # entries ìƒì„¸ ì •ë³´
            if feed.entries:
                print(f"âœ… First entry keys: {list(feed.entries[0].keys()) if feed.entries else 'No entries'}")
                print(f"âœ… First entry title: {getattr(feed.entries[0], 'title', 'No title') if feed.entries else 'No entries'}")
            else:
                print(f"âŒ No entries found in feed")

            articles = []
            for entry in feed.entries[:20]:  # ìµœëŒ€ 20ê°œ ë‰´ìŠ¤
                # ì´ë¯¸ì§€ URL ì¶”ì¶œ ê°œì„ 
                image_url = ""
                if hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
                    image_url = entry.media_thumbnail[0].get('url', '')
                elif hasattr(entry, 'media_content') and entry.media_content:
                    image_url = entry.media_content[0].get('url', '')
                elif hasattr(entry, 'enclosures') and entry.enclosures:
                    for enclosure in entry.enclosures:
                        if enclosure.get('type', '').startswith('image/'):
                            image_url = enclosure.get('url', '')
                            break

                # ë‚ ì§œ ì²˜ë¦¬ ê°œì„ 
                published_at = getattr(entry, 'published', '')
                if published_at:
                    try:
                        from email.utils import parsedate_to_datetime
                        published_at = parsedate_to_datetime(published_at).isoformat()
                    except:
                        published_at = datetime.now().isoformat()

                # Google News ë§í¬ì—ì„œ ì‹¤ì œ ë‰´ìŠ¤ URL ì¶”ì¶œ ì‹œë„
                actual_url = self._extract_real_url(entry.link)

                article = {
                    "title": entry.title,
                    "description": getattr(entry, 'summary', ''),
                    "content": getattr(entry, 'summary', ''),  # RSSì—ì„œëŠ” ì½˜í…ì¸ ê°€ ì œí•œì 
                    "url": actual_url,  # ì‹¤ì œ ë‰´ìŠ¤ URL ì‚¬ìš©
                    "urlToImage": image_url,
                    "publishedAt": published_at
                }
                articles.append(article)

            print(f"âœ… Returning {len(articles)} articles")
            return articles

        except Exception as e:
            print(f"ğŸ’¥ Error parsing RSS feed for {topic}: {e}")
            import traceback
            print(f"ğŸ’¥ Full traceback: {traceback.format_exc()}")
            return []
        







# News fetch, save Func
async def fetch_and_store_news(db: Session):
    """Google News RSSì—ì„œ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì™€ì„œ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
    client = GoogleNewsRSSClient()

    # ì—¬ëŸ¬ ì¹´í…Œê³ ë¦¬ì˜ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
    categories = ["business", "technology", "science", "health", "entertainment"]

    total_processed = 0
    total_saved = 0

    for category in categories:
        print(f"ğŸ” Fetching {category} news...")  # ë””ë²„ê¹… ë¡œê·¸
        articles = client.get_news_by_topic(topic=category)
        print(f"ğŸ“Š Found {len(articles)} articles for {category}")  # ë””ë²„ê¹… ë¡œê·¸

        # ìµœì‹ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  5ê°œë¡œ ì œí•œ
        print(f"ğŸ”¢ Before sorting: {len(articles)} articles")  # ë””ë²„ê¹… ë¡œê·¸
        try:
            articles = sorted(articles, key=get_sort_key, reverse=True)[:5]
            print(f"âœ… After sorting and limiting: {len(articles)} articles")  # ë””ë²„ê¹… ë¡œê·¸
        except Exception as sort_err:
            print(f"âŒ Sorting failed: {sort_err}")  # ë””ë²„ê¹… ë¡œê·¸
            # ì •ë ¬ ì‹¤íŒ¨ì‹œ ê·¸ëƒ¥ ì²˜ìŒ 5ê°œ ì‚¬ìš©
            articles = articles[:5]
        print(f"ğŸ“Š Processing {len(articles)} most recent articles for {category}")  # ë””ë²„ê¹… ë¡œê·¸

        try:

            for i, article in enumerate(articles):
                title = article.get("title", "").strip()
                description = article.get("description", "").strip()

                # HTML íƒœê·¸ ì œê±°ë§Œ í•˜ê³  ë
                if description:
                    soup = BeautifulSoup(description, 'html.parser')
                    description = soup.get_text().strip()
                    description = ' '.join(description.split())

                total_processed += 1
                print(f"ğŸ“° Processing article {i+1}: {title[:50]}...")

                # ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„
                news_url = article.get("url", "")
                content = description  # ê¸°ë³¸ê°’ìœ¼ë¡œ RSS ìš”ì•½ ì‚¬ìš©

                # ì‹¤ì œ ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„
                if news_url:
                    try:
                        extracted_content = client.extract_article_content(news_url)
                        if extracted_content and len(extracted_content.strip()) > 50:
                            content = extracted_content
                            print(f"âœ… ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ: {len(content)}ì")
                        else:
                            print("âš ï¸ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨, RSS ìš”ì•½ ì‚¬ìš©")
                    except Exception as e:
                        print(f"ğŸ’¥ ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}, RSS ìš”ì•½ ì‚¬ìš©")

                # ì¤‘ë³µ ì²´í¬ ê°„ë‹¨í•˜ê²Œ
                existing = db.query(Post).filter(Post.title == title).first()
                if existing:
                    print(f"ğŸ”„ Skipped: Already exists - {title[:30]}...")
                    continue

                # ì €ì¥
                full_content = content
                if news_url:
                    full_content += f"\n\nğŸ”— ì „ì²´ ê¸°ì‚¬ ë³´ê¸°: {news_url}"

                image_url = article.get("urlToImage", "")
                if not image_url:
                    image_url = "https://images.unsplash.com/photo-1504711434969-e33886168f5c?auto=format&fit=crop&q=80&w=800"

                post_data = {
                    "title": title[:200],
                    "summary": description[:300],
                    "content": full_content,
                    "category": category.capitalize(),
                    "image_url": image_url
                }

                db_post = Post(**post_data)
                db.add(db_post)
                total_saved += 1
                print(f"âœ… Saved article: {title[:30]}...")
                
        except Exception as e:
            print(f"ğŸ’¥ Error fetching {category} news: {e}")
            continue

    try:
        db.commit()
        print(f"ğŸ‰ Total processed: {total_processed}, Total saved: {total_saved}")  # ìµœì¢… ê²°ê³¼ ë¡œê·¸
        print("News fetched and stored successfully")
    except Exception as e:
        db.rollback()
        print(f"ğŸ’¥ Error saving news to database: {e}")

# API ì•¤ë“œ í¬ì¸íŠ¸ë“¤
@app.get("/api/posts", response_model=List[PostResponse])
async def get_posts(
    category: Optional[str] = Query(None),
    search: Optional[str] = Query(None),
    db: Session = Depends(get_db)
):
    query = db.query(Post)

    if category:
        query = query.filter(Post.category == category)

    if search:
        search_term = f"%{search.lower()}%"
        query = query.filter(
            (Post.title.ilike(search_term)) |
            (Post.content.ilike(search_term))
        )

    posts = query.order_by(Post.created_at.desc()).all()
    return posts

# FastAPIì—ì„œëŠ” ê²½ë¡œ íŒŒë¼ë¯¸í„°ë¥¼ ì¤‘ê´„í˜¸ë¡œ ì„ ì–¸í•´ì•¼ í•˜ë©°, f-stringì„ ì‚¬ìš©í•  í•„ìš”ê°€ ì—†ë‹¤.
@app.api_route("/api/posts/{post_id}", methods=["GET"])  # api_routeë¡œ ë³€ê²½í•˜ì—¬ validation ìš°íšŒ
async def get_post(post_id, db: Session = Depends(get_db)):  # íƒ€ì… íŒíŠ¸ ì œê±°
    print(f"DEBUG: Requesting post with ID: {post_id}, type: {type(post_id)}")

    try:
        post_id_int = int(post_id)
        print(f"DEBUG: Converted to int: {post_id_int}")
    except ValueError as e:
        print(f"DEBUG: Failed to convert ID to int: {e}")
        raise HTTPException(status_code=400, detail=f"Invalid ID format: {post_id}")

    # ë°ì´í„°ë² ì´ìŠ¤ì— í•´ë‹¹ IDê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    all_posts = db.query(Post).all()
    print(f"DEBUG: All post IDs in database: {[p.id for p in all_posts]}")

    post = db.query(Post).filter(Post.id == post_id_int).first()
    if not post:
        print(f"DEBUG: Post with ID {post_id_int} not found")
        raise HTTPException(status_code=404, detail="Post not found")

    print(f"DEBUG: Found post: {post.id}, {post.title}")
    return post


@app.post("/api/posts", response_model=PostResponse, status_code=201)
async def create_post(post: PostCreate, db: Session = Depends(get_db)):
    db_post = Post(**post.dict())
    db.add(db_post)
    db.commit()
    db.refresh(db_post)
    return db_post


@app.post("/api/news/fetch")
async def fetch_latest_news(db: Session = Depends(get_db)):
    """ìµœì‹  ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì™€ì„œ ì €ì¥"""
    await fetch_and_store_news(db)
    return {"message": "Latest news fetched and stored successfully"}
    
        

# ê°„ë‹¨í•œ ì„œë²„ ì‹¤í–‰
if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        print("ğŸ§ª ë³¸ë¬¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸...")
        test_url = 'https://news.google.com/rss/articles/CBMiVkFVX3lxTE9WUjlNZ0psX0hZMW5mVlQyZFhRblQ4TVFaRVdUMmdIMXNKbXUzZ284MmVuWDhRcVV6eFBHdWWhmMkhON1lEMFRwWnMxNDdMMU1Qb3BsdEZB?oc=5'
        try:
            result = extract_news_content(test_url)
            if result:
                print(f'âœ… ì„±ê³µ! ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(result)}')
                print(f'ğŸ“ ë¯¸ë¦¬ë³´ê¸°: {result[:200]}...')
            else:
                print('âŒ ì¶”ì¶œ ì‹¤íŒ¨')
        except Exception as e:
            print(f'ğŸ’¥ ì˜¤ë¥˜ ë°œìƒ: {e}')
            import traceback
            print(traceback.format_exc())
    else:
        port = int(os.getenv("PORT", 8000))
        uvicorn.run(app, host="127.0.0.1", port=port)



